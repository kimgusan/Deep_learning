{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.6 재현 신경망(Recurrent Neural Network) 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 파일 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "TRAIN_INPUT_DATA = 'train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.load(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'rb'))\n",
    "label_data = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'rb'))\n",
    "prepro_configs = None\n",
    "\n",
    "with open(DATA_IN_PATH + DATA_CONFIGS, 'r') as f:\n",
    "    prepro_configs =json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.1\n",
    "RANDOM_SEED = 13371447\n",
    "\n",
    "input_train, input_eval, label_train, label_eval = train_test_split(input_data, label_data, test_size = TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))\n",
    "    dataset = dataset.shuffle(buffer_size=50000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "# 데이터셋 정의\n",
    "train_dataset = train_input_fn()\n",
    "eval_dataset = eval_input_fn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 값 정의\n",
    "VOCAB_SIZE = prepro_configs['vocab_size']+1\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "HIDDEN_STATE_DIM = 150\n",
    "DENSE_FEATURE_DIM = 150\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74065 74066\n"
     ]
    }
   ],
   "source": [
    "print(len(prepro_configs['vocab']), VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 함수\n",
    "def create_model(vocab_size, embedding_dim, hidden_dim, dense_dim):\n",
    "    # Sequential 모델을 정의합니다.\n",
    "    model = tf.keras.Sequential([\n",
    "        # Embedding 층: 단어를 고정된 크기의 벡터로 변환합니다.\n",
    "        # vocab_size: 어휘 사전의 크기\n",
    "        # embedding_dim: 임베딩 벡터의 차원\n",
    "        # input_length: 입력 시퀀스의 길이 (None으로 설정 시 가변 길이 입력 허용)\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=None),\n",
    "\n",
    "        # Dropout 층: 과적합을 방지하기 위해 20%의 뉴런을 무작위로 비활성화합니다.\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "        # LSTM 층: 첫 번째 LSTM 층으로, return_sequences=True로 설정되어 모든 타임스텝의 출력을 반환합니다.\n",
    "        # hidden_dim: LSTM의 출력 차원\n",
    "        tf.keras.layers.LSTM(hidden_dim, return_sequences=True),\n",
    "\n",
    "        # LSTM 층: 두 번째 LSTM 층으로, 마지막 타임스텝의 출력만 반환합니다.\n",
    "        tf.keras.layers.LSTM(hidden_dim),\n",
    "\n",
    "        # Dropout 층: 과적합을 방지하기 위해 다시 20%의 뉴런을 무작위로 비활성화합니다.\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "        # Dense 층: 완전 연결층으로, tanh 활성화 함수를 사용하여 dense_dim 차원의 출력을 생성합니다.\n",
    "        tf.keras.layers.Dense(dense_dim, activation='tanh'),\n",
    "\n",
    "        # Dropout 층: 과적합을 방지하기 위해 또다시 20%의 뉴런을 무작위로 비활성화합니다.\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "        # Dense 층: 최종 출력층으로, sigmoid 활성화 함수를 사용하여 이진 분류 결과를 출력합니다.\n",
    "        # 1개의 유닛을 가지며, 0~1 사이의 값을 출력합니다.\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # 모델 컴파일: 옵티마이저, 손실 함수, 평가 지표 설정\n",
    "    # Adam 옵티마이저 사용, 손실 함수로 이진 교차 엔트로피 사용, 정확도를 평가 지표로 설정\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # 모델 반환\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_api.py:100: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  saving_lib.save_model(model, filepath)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터 경로가 존재하지 않으면 생성\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "# 모델 생성\n",
    "model = create_model(VOCAB_SIZE, WORD_EMBEDDING_DIM, HIDDEN_STATE_DIM, DENSE_FEATURE_DIM)\n",
    "\n",
    "# 모델 구조 확인\n",
    "model.summary()\n",
    "\n",
    "# 모델 저장\n",
    "model.save(os.path.join(DATA_OUT_PATH, 'rnn_model.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow 1.x 에서 사용\n",
    "\n",
    "# def model_fn(features, labels, mode):\n",
    "#     TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "#     EVAL = mode == tf.estimator.Modelkeys.EVAL\n",
    "#     PREDICT = mode == tf.estimator.Modelkeys.PREDICT\n",
    "\n",
    "#     # feature의 경우 딕셔너리 형태로 구성되어 있기 때문.\n",
    "#     # tf.keras.layers.Embedding 함수가 해당 임베딩 층(Embedding Layer)를 거치도록 도와주는 함수이다.\n",
    "#     # 임베딩 층의 경우 단어의 수치적 표현, 차원축소, 유사성 학습을 진행할 수 있게 도와주는 층이라고 보면 된다.\n",
    "#     embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)(features['x'])\n",
    "#     embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)\n",
    "\n",
    "#     # 순환 신경망을 사용하기 위한 함수\n",
    "#     # 해당 객체는 하나의 LSTM Cell 을 의미하며 해당 Cell 객체를 여러개 생성해서 하나의 리스트로 만들어 준다.\n",
    "#     rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]]\n",
    "#     # LTSMCell을 쌓게 되면 하나의 MultiRNN을 묶어야, 즉 래핑(wrapping)을 해야한다. 해당 multiRNNCell을 생성함으로써 스택 구조의 LSTM 신경망을 구현할 수 있다.\n",
    "#     multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "\n",
    "#     # 네트워크와 임베딩 벡터와 연산하기 위해 dynamic_rnn 함수를 선언한다.\n",
    "#     # 텐서플로의 dynamic_rnn: for 구문 활용없이 자동으로 순환신경망을 만들어 주는 역할을 한다\n",
    "#     outputs, state = tf.nn.dynamic_rnn(cell = multi_rnn_cell,\n",
    "#                                        inputs=embedding_layer,\n",
    "#                                        dtype = tf.float32)\n",
    "\n",
    "#     outputs = tf.keras.layers.Dropout(0.2)(outputs)\n",
    "#     hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation=tf.nn.tanh)(outpus[:, -1, :])\n",
    "#     hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer)\n",
    "#     # 최종적으로 1dim의 output만 할 수 있도록 dense layer 를 활용해 차원 변환을 한다\n",
    "#     logits = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "#     logits = tf.squeeze(logits, axis = -1)\n",
    "\n",
    "#     loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "\n",
    "#     # 훈련 데이터에 대하여 경사하강법을 진행하는 부분\n",
    "#     if TRAIN:\n",
    "#         global_step = tf.train.get_global_step()\n",
    "#         # 경사하강법 중 하나인 아담 옵티마이저를 활용하여 경사하강을 진행\n",
    "#         train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "\n",
    "#         return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)\n",
    "\n",
    "#     # 모델 출력값에 대하여 0~ 1 사이의 값으로 정의하기 위해 sigmoid 함수를 사용\n",
    "#     sigmoid_logits = tf.nn.sigmoid(logits)\n",
    "\n",
    "#     # 모델 평가에 대한 코드\n",
    "#     if EVAL:\n",
    "#         accuracy = tf.metrics.accuracy(laels, tf.round(sigmoid_logits))\n",
    "#         eval_metric_ops = {'acc':accuracy}\n",
    "\n",
    "#         return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "        \n",
    "#     # 모델 예측을 위한 코드\n",
    "#     if PREDICT:\n",
    "#         # 모델 출력값에 대하여 0~ 1 사이의 값으로 정의하기 위해 sigmoid 함수를 사용\n",
    "#         predictions = {'sentiment': sigmoid_logits}\n",
    "\n",
    "#         return tf.estimator.EstimatorSpec(mode = mode, predictions = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(DATA_OUT_PATH):\n",
    "#     os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "# est = tf.estimator.Estimator(model_fn=model_fn,\n",
    "#                              model_dir=DATA_OUT_PATH + 'checkpoint/rnn') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 180ms/step - accuracy: 0.5772 - loss: 0.6356 - val_accuracy: 0.8720 - val_loss: 0.3344\n",
      "Epoch 2/3\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 180ms/step - accuracy: 0.9627 - loss: 0.1119 - val_accuracy: 0.8572 - val_loss: 0.5427\n",
      "Epoch 3/3\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 183ms/step - accuracy: 0.9949 - loss: 0.0244 - val_accuracy: 0.8556 - val_loss: 0.8312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x32c5164d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "\n",
    "# # 모델 훈련\n",
    "# model.fit(input_train, label_train, epochs=10, validation_data=(input_eval, label_eval))\n",
    "\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=eval_dataset)\n",
    "\n",
    "# tensorflow 1.x 에서 사용\n",
    "# est.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 캐글 평가 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "\n",
    "test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = tf.data.Dataset.from_tensor_slices({'x': test_input_data})\n",
    "# test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 50ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_input_data)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# predictions는 이제 예측 결과를 포함합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\":test_input_data}, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.array([p['sentiment'] for p in est.predict(input_fn=predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9961269e-01],\n",
       "       [3.9450437e-04],\n",
       "       [2.1139032e-03],\n",
       "       ...,\n",
       "       [4.4497382e-04],\n",
       "       [9.9961323e-01],\n",
       "       [5.8831203e-01]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow 2.x 코드\n",
    "predictions = np.array(predictions)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb', ), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\": list(test_id), \"sentiment\":list(predictions)} )\n",
    "output.to_csv(DATA_OUT_PATH + 'movie_review_result_rnn.csv', index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
