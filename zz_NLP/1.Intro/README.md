# 1. Introduction

해당 과정은 tensorflow 1.x 기준으로 작성되었지만 앞에 s 가 붙은 항목은 tensorflow 2.x 에서도 작동할 수 있도록 수정된 코드 입니다.

### tip. embeding layer 
- 임베딩 레이어 사용 이유
1. 고차원 원-핫 인코딩을 줄이기 위해:

텍스트 데이터를 원-핫 인코딩으로 표현하면, 각 단어는 매우 고차원 벡터로 표현됩니다. 예를 들어, 어휘 크기가 10,000인 경우 각 단어는 10,000차원의 벡터로 표현됩니다. 이는 메모리 사용 측면에서 비효율적일 뿐만 아니라, 계산 비용도 높습니다.
임베딩 레이어는 단어를 저차원 벡터로 표현할 수 있게 합니다. 예를 들어, 10,000차원의 원-핫 벡터 대신 128차원 벡터로 단어를 표현할 수 있습니다. 이 벡터는 임베딩 공간에서 학습된 의미를 가지게 됩니다.

2. 단어 간 의미적 유사성을 학습하기 위해:

임베딩 레이어는 단어를 저차원 벡터 공간에 매핑하는데, 이 벡터 공간에서 비슷한 의미를 가진 단어들이 서로 가깝게 위치하도록 학습됩니다. 예를 들어, "king"과 "queen"은 임베딩 공간에서 유사한 위치에 놓이게 됩니다.
이는 모델이 단어 간의 의미적 관계를 학습할 수 있게 하며, 텍스트의 의미를 더 잘 이해하게 만듭니다.

3. 데이터 차원의 축소 및 계산 효율성 향상:

임베딩 레이어는 단어를 비교적 작은 벡터로 변환하여 입력으로 사용하기 때문에, 계산 효율성을 크게 향상시킵니다. 이를 통해 모델은 더 적은 리소스로 텍스트 데이터를 처리할 수 있습니다.

4. 학습 가능한 표현:

임베딩 레이어는 학습 가능한 파라미터입니다. 모델이 학습하는 동안 임베딩 레이어도 함께 학습되어, 주어진 문제에 최적화된 단어 벡터를 생성할 수 있습니다. 이를 통해 모델의 성능이 향상됩니다.

### 군집화 (비지도 학습)
- k-평균 군집:
데이터 안에서 대표하는 군집의 중심을 찾는 알고리즘이다. 알고리즘은 계속해서 반복적으로 수행되는데, 우선 k개 만큼의 중심을 임의로 설정한다. 그러고 난 후, 모든 데이터를 가장 가까운 중심에 할당하여 같은 중심에 할당된 데이터들을 하나의 군집으로 판단한다. 각 군집 내 데이터들을 가지고 군집의 중심을 새로 구해서 업데이트하고 그 과정이 반복된다.

### 텍스트 특징 확인
1. CountVectorizer
2. TfidfVectorizer
3. HashingVectorizer

### 텍스트 특징 추출 (라이브러리)
1. 영문 토크나이징 nltk, spacy
2. 한글 토크나이징 라이브러리 KoNLPy