## NLP(Natural Language Processing): 자연어 처리

-   1970년대 과학자들이 통계적 모델을 사용하여 자연어 텍스트를 분석하고 생성하는 통계적 NLP를 규칙 기반 접근법의 대안으로 사용하기 시작.

1. AI 기반 NLP: 머신러닝 알고리즘과 기술을 사용하여 인간 언어를 처리, 이해 및 생성하는 것을 포함.
2. 규칙 기반 NLP: 언어 데이터를 분석하고 생성하는 데 사용할 수 있는 규칙 또는 패턴 집합을 만드는 작업이 포함.
3. 통계적 NLP: 언어를 분석하고 예측하기 위해 대규모 데이터 세트에서 파생된 통계 모델을 사용하는 것을 포함.
4. 하이브리드 NLP: 위의 세가지 접근 방식을 결합.

### NLP의 작동 방식.

> 1단계: 데이터 전처리
>
> 1. 대량의 텍스트를 사용하여 데이터로 분할하는 텍스트 마이닝 또는 텍스트를 개별 단위로 분할하는 토큰화 포함.
> 2. 불용어 제거: 분석에 도움이 되지 않는 일반적인 단어와 관사를 제거하는 도구.
> 3. 어간 추출과 표제어 추출: 단어를 기본 어근 형태로 분해하여 의미를 쉽게 식별.
> 4. 품사 태그: 문장에서 명사, 동사, 형용사 및 다른 품사를 식별.
> 5. 구문 분석: 문장의 구조와 서로 다른 단어들이 어떻게 연관되어 있는지 분석.

> 2단계: 알고리즘 개발
>
> 1. 정서 분석: 텍스트의 정서적인 어조 또는 정서를 결정합니다. 단어, 구문 및 표현식을 긍정, 부정, 중립으로 레이블 지정.
> 2. 명명된 \*엔터티 인식: 사용자, 위치, 날짜 및 조직과 같은 명명된 엔터니를 식별하고 분류.
> 3. 기계 변역: 머신러닝을 사용하여 한 언어에서 다른 언어로 텍스트를 자동으로 변역, 특정 맥락에서 일련의 단어의 가능성을 예측
> 4. 언어 모델링: 자동 완성, 자동 수정 애플리케이션 및 음성-텍스트 시스템에 사용됩니다.
>     > - 엔터티: 업무에 필요하고 유용한 정보를 저장하고 관리하기 위한 집합적인 것(Thing), 대상들 간에 동질성을 지닌 인스턴스들이나 그들이 행하는 행위의 집합으로 정의할 수 있다.

### NLP의 두가지 분기

1. 자연어 이해(NLU): 컴퓨터가 인간이 사용하는 유사한 도구를 사용하여 인간의 언어를 이해할 수 있도록 하는데 중점.  
   컴퓨터가 문맥, 의도, 정서 및 모호성을 포함하여 **인간 언어의 뉘앙스를 이해**할 수 있도록 하는 것이 목표.
2. 자연어 생성(NLG): 데이터베이스 또는 일련의 규칙에서 인간과 같은 언어를 만드는데 중점.  
   NLG의 목표는 **인간이 쉽게 이해할 수 있는 텍스트를 만드는 것**.

## RNN(Recurrent Neural Network): 순환 재귀 신경망

-   입력과 출력을 시퀸스 단위로 처리하는 시퀸스(Sequence)모델입니다.
-   RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고 있습니다.

### 메모리 셀

-   시퀀스 데이터의 과거 정보를 저장하는 역할을 합니다.

### 종류

1. 일 대 다(one-to-many)

-   하나의 입력에 대해 여러 개의 출력을 가지는 일 대 다 모델을 나타냅니다.
-   ex) **이미지 캡셔닝**

2. 다 대 일(many-to-one)

-   시퀸스 입력에 대해서 하나의 출력을 하는 다 대 일 모델을 나타닙니다.
-   ex) **감성 분류, 스팸 메일 분류**

3. 다 대 대(many-to-many)

-   시퀸스 입력에 대해 시퀸스 출력를 하는 다 대 다 모델을 나타냅니다.
-   ex) **챗봇, 번역기, 개체명 인식, 품사 태깅**

### RNN 학습

-   RNN 학습에는 back propagation의 확장인 BPTT(Back Propagation Through Time) 이 사용됩니댜.
-   BPTT: 재귀적인 형태의 모델을 시간에 대해 펼쳐서 현재 시점의 에러를 최초 시점까지 전파해 학습하는 것.
    > 이렇게 학습 시에 앞의 시점으로 계속 전파가 일어나는데,  
    >  **전파가 길어질수록 기울기 값이 0에 가까워지는 기울기 소실 문제가 발생하게 됩니다.**

#### 바닐라 RNN의 한계(가장 단순현 형태를 바닐라 RNN 이라고 표현)

-   **장기 의존성**: 앞의 정보가 뒤로 충분이 전달되지 못하는 현상이 발생된다
    > 이러한 문제를 보완한 RNN 모델로  
    > 장단기 메모리(LSTM)과 게이트 순환 유닛 (GRU)가 있습니다.

## LSTM(Long Short-Term Memory, LSTM): 장단기 메모리

-   LSTM은 은닉층의 메모리 셀에 삭제 게이트, 입력 게이트, 출력 게이트를 추가하여 필요하지 않은 기억은 지우고, 기억해야 할 정보를 저장합니다.
-   바닐라 RNN에 비해 은닉 상태를 계산하는 식이 더 복잡해졌으며 **셀 상태**라는 값을 추가했습니다.

## GRU(Gated Recurrent Unit)

### GRU의 특징

0. LTSM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트는 계산을 줄인 모델
1. 간단한 구조: GRU는 LSTM보다 구조가 간단하여 계산 비용이 적습니다. LSTM은 입력 게이트, 포겟 게이트, 출력 게이트의 세 가지 게이트를 사용하는 반면, GRU는 업데이트 게이트와 리셋 게이트 두 가지를 사용합니다.
2. 성능: GRU는 LSTM과 유사한 성능을 발휘하면서도 학습이 더 빠르고 메모리 사용이 적습니다.
3. 적용: GRU는 자연어 처리(NLP), 시계열 예측, 음성 인식 등 다양한 시퀀스 데이터 처리 문제에서 널리 사용됩니다.
