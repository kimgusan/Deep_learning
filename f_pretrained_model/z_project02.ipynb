{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f1c9dc-13f7-42f0-9901-109ee0abcca0",
   "metadata": {},
   "source": [
    "# ⛅️ Project 02: 날씨 데이터 분류\n",
    "---\n",
    "## 주제: 날씨 데이터 분류 (사전 훈련 모델 or 미세 조정(sequence) , (저용량))\n",
    "---\n",
    "### 목표 (Target)\n",
    "- **흐림**\n",
    "- **비**\n",
    "- **맑음⛅️**\n",
    "- **해돋이**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faae42c-fec2-4f2c-ae72-49ad708fb70d",
   "metadata": {},
   "source": [
    "### 목차\n",
    "1. 이미지 불러오기.\n",
    "2. 데이터 프레임으로 file 경로 및 target 분류.\n",
    "3. 데이터셋 미세 조정을 통한 데이터 증강 및 메모리 효율성 확보.\n",
    "4. 데이터 훈련.\n",
    "5. 데이터 검증.\n",
    "6. 결론."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4a3aa-3912-400b-b91d-885aac5c86fe",
   "metadata": {},
   "source": [
    "## 1. 이미지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3a44b5-eacc-4a0f-aa34-784b01263db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1125 images belonging to 4 classes.\n",
      "{'Cloudy': 0, 'Rain': 1, 'Shine': 2, 'Sunrise': 3}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "root = './datasets/p_Multi-class Weather Dataset/'\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "idg = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "generator = idg.flow_from_directory(root, target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "print(generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9ba7bf-af15-493a-9695-b611c9ef4c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Cloudy', 1: 'Rain', 2: 'Shine', 3: 'Sunrise'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_name = {v: k for k, v in generator.class_indices.items()}\n",
    "target_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70dea256-c055-47a1-9d61-b22c3f5dc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = []\n",
    "for target in generator.classes:\n",
    "    target_names.append(target_name[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "683d9fd2-a19a-43d0-8649-39c3f520806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_paths</th>\n",
       "      <th>target_names</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Cloud...</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Cloud...</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Cloud...</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Cloud...</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Cloud...</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Sunri...</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Sunri...</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Sunri...</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Sunri...</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>./datasets/p_Multi-class Weather Dataset/Sunri...</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file_paths target_names  targets\n",
       "0     ./datasets/p_Multi-class Weather Dataset/Cloud...       Cloudy        0\n",
       "1     ./datasets/p_Multi-class Weather Dataset/Cloud...       Cloudy        0\n",
       "2     ./datasets/p_Multi-class Weather Dataset/Cloud...       Cloudy        0\n",
       "3     ./datasets/p_Multi-class Weather Dataset/Cloud...       Cloudy        0\n",
       "4     ./datasets/p_Multi-class Weather Dataset/Cloud...       Cloudy        0\n",
       "...                                                 ...          ...      ...\n",
       "1120  ./datasets/p_Multi-class Weather Dataset/Sunri...      Sunrise        3\n",
       "1121  ./datasets/p_Multi-class Weather Dataset/Sunri...      Sunrise        3\n",
       "1122  ./datasets/p_Multi-class Weather Dataset/Sunri...      Sunrise        3\n",
       "1123  ./datasets/p_Multi-class Weather Dataset/Sunri...      Sunrise        3\n",
       "1124  ./datasets/p_Multi-class Weather Dataset/Sunri...      Sunrise        3\n",
       "\n",
       "[1125 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "w_df = pd.DataFrame({'file_paths': generator.filepaths,'target_names': target_names, 'targets': generator.classes})\n",
    "w_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b071f9c-3acb-40e1-8826-d85d35737b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets\n",
      "3    286\n",
      "0    240\n",
      "2    202\n",
      "1    172\n",
      "Name: count, dtype: int64\n",
      "targets\n",
      "3    71\n",
      "0    60\n",
      "2    51\n",
      "1    43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, test_images, train_targets, test_targets = \\\n",
    "train_test_split(w_df.file_paths, \n",
    "                 w_df.targets, \n",
    "                 stratify=animal_df.targets, \n",
    "                 test_size=0.2, random_state=124)\n",
    "\n",
    "print(train_targets.value_counts())\n",
    "print(test_targets.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d6e865-31e2-45c9-8d2d-398bf2c638b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets\n",
      "3    229\n",
      "0    192\n",
      "2    162\n",
      "1    137\n",
      "Name: count, dtype: int64\n",
      "targets\n",
      "3    57\n",
      "0    48\n",
      "2    40\n",
      "1    35\n",
      "Name: count, dtype: int64\n",
      "targets\n",
      "3    71\n",
      "0    60\n",
      "2    51\n",
      "1    43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_images, validation_images, train_targets, validation_targets = \\\n",
    "train_test_split(train_images, train_targets, stratify = train_targets, test_size=0.2, random_state=124)\n",
    "\n",
    "print(train_targets.value_counts())\n",
    "print(validation_targets.value_counts())\n",
    "print(test_targets.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad748247-1d8a-4db1-9824-0127a3557852",
   "metadata": {},
   "source": [
    "## 2. 데이터 프레임 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3da758b-88fb-4351-a637-18ef2570e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 3)\n",
      "(180, 3)\n",
      "(225, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = w_df.iloc[train_images.index].reset_index(drop=True)\n",
    "validation_df = w_df.iloc[validation_images.index].reset_index(drop=True)\n",
    "test_df = w_df.iloc[test_images.index].reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(validation_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e402e7-836e-4069-99fe-84efd1b0947d",
   "metadata": {},
   "source": [
    "## 3. 이미지의 개수가 적기 때문에 데이터셋 미세 조정 및 albumentation.\n",
    "- (ShiftScaleRotate, HorizontalFlip, RandomBrightnessContrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b48d17-96d7-458a-b6f8-ceaa1fe6bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class Dataset(Sequence):\n",
    "    def __init__(self, file_paths, targets, batch_size = BATCH_SIZE, aug=None, preprocess=None, shuffle=False):\n",
    "        self.file_paths = file_paths\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.aug = aug\n",
    "        self.preprocess = preprocess\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        if self.shuffle:\n",
    "            self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.targets) / self.batch_size)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_paths_batch = self.file_paths[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        targets_batch = self.targets[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "\n",
    "        results_batch = np.zeros((file_paths_batch.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "        for i in range(file_paths_batch.shape[0]):\n",
    "            image = cv2.cvtColor(cv2.imread(file_paths_batch[i]), cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "            if self.aug is not None:\n",
    "                image = self.aug(image=image)['image\n",
    "    \n",
    "            if self.preprocess is not None:\n",
    "                image = self.preprocess(image)\n",
    "\n",
    "            results_batch[i] = image\n",
    "\n",
    "        return results_batch, targets_batch\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.file_paths, self.targets = shuffle(self.file_paths, self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbfc65-45ae-481d-94cf-8116c5afc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from tensorflow.keras.applicatios.xception import preprocess_imput as xception_preprocess_input\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_file_paths = train_df['file_paths'].values\n",
    "train_targets = pd.get_dummies(train_df['targets']).values\n",
    "\n",
    "validation_file_paths = validation_df['file_paths'].values\n",
    "validation_targets = pd.get_dummies(validation_df['targets']).values\n",
    "\n",
    "test_file_paths = test_df['file_paths'].values\n",
    "test_targets = pd.get_dummies(test_df['targets']).values\n",
    "\n",
    "\n",
    "aug = A.Compose([\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, p=0.5)\n",
    "],p=0.7)\n",
    "\n",
    "\n",
    "train_dataset = Dataset(train_file_paths, \n",
    "                        train_targets, \n",
    "                        batch_size = BATCH_SIZE,\n",
    "                        aug = aug\n",
    "                        preprocess = xception_preprocess_input, \n",
    "                        shuffle= True)\n",
    "\n",
    "train_dataset = Dataset(validation_file_paths, \n",
    "                        validation_targets, \n",
    "                        batch_size = BATCH_SIZE,\n",
    "                        aug = aug\n",
    "                        preprocess = xception_preprocess_input)\n",
    "\n",
    "test_dataset = Dataset(test_file_paths, \n",
    "                        test_targets, \n",
    "                        batch_size = BATCH_SIZE,\n",
    "                        aug = aug\n",
    "                        preprocess = xception_preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b89ede-d0be-40e9-be30-dd2bd8db9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Dropout, Flatten, Activation, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "def create_model(model_name='vgg16', verbose=False):\n",
    "    input_tensor = Input(shape = (IMAGE_SIZE, IMAGE_SIZE), 3)\n",
    "    if model_name == 'vgg16':\n",
    "        model = VGG16(input_tensor=input_tensor, include_top=False, weight= 'imagenet')\n",
    "    elif model_name == 'resnet50':\n",
    "        model = ResNet50V2(input_tensor=input_tensor, include_top=False, weight= 'imagenet')\n",
    "    elif model_name == 'xception':\n",
    "        model = Xception(input_tensor=input_tensor, include_top=False, weight= 'imagenet')\n",
    "\n",
    "    x = model.output\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    if model_name != 'vgg16':\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    if model_name != 'vgg16':\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(4, activation='softmax', name ='output')(x)\n",
    "\n",
    "    model = Model(inputs = input_tensor, outputs = output)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b55e8-a914-42bc-be87-4de43c22c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=\"./callback_files/project02/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf302130-72a8-479d-99d3-2c9d23c2162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "model = create_model(model_name='xception', verbose=True)\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b49074-8bc1-4baf-adba-f4aa65c1df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=N_EPOCHS, \n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=[mcp_cb, rlr_cb, ely_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe123f2-1139-45c3-a72e-3bc2dcd435c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a9a51-f9aa-4202-aaef-d02f3b3f5be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
