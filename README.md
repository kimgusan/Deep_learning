# Deep Learning

-   ì¸ê³µ ì‹ ê²½ë§(Artivicial Neural Network)ì˜ ì¸µì„ ì—°ì†ì ìœ¼ë¡œ ê¹Šê²Œ ìŒ“ì•„ì˜¬ë ¤ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì˜ë¯¸í•œë‹¤.
-   ì¸ê°„ì´ í•™ìŠµí•˜ê³  ê¸°ì–µí•˜ëŠ” ë§¤ì»¤ë‹ˆì¦˜ì„ ëª¨ë°©í•œ ê¸°ê³„í•™ìŠµì´ë‹¤.
-   ì¸ê°„ì€ í•™ìŠµ ì‹œ, ë‡Œì— ìˆëŠ” ë‰´ëŸ°ì´ ìê·¹ì„ ë°›ì•„ë“¤ì—¬ì„œ ì¼ì • ìê·¹ ì´ìƒì´ ë˜ë©´, í™”í•™ë¬¼ì§ˆì„ í†µí•´ ë‹¤ë¥¸ ë‰´ëŸ°ê³¼ ì—°ê²°ë˜ë©° í•´ë‹¹ ë¶€ë¶„ì´ ë°œë‹¬í•œë‹¤.
-   ìê·¹ì´ ì•½í•˜ê±°ë‚˜ ê¸°ì¤€ì¹˜ë¥¼ ë„˜ì§€ ëª»í•˜ë©´, ë‰´ëŸ°ì€ ì—°ê²°ë˜ì§€ ì•ŠëŠ”ë‹¤.
-   ì…ë ¥í•œ ë°ì´í„°ê°€ í™œì„± í•¨ìˆ˜ì—ì„œ ì„ê³„ì ì„ ë„˜ê²Œ ë˜ë©´ ì¶œë ¥ëœë‹¤.
-   ì´ˆê¸° ì¸ê³µ ì‹ ê²½ë§(Perceptron)ì—ì„œ ê¹Šê²Œ ì¸µì„ ìŒ“ì•„ í•™ìŠµí•˜ëŠ” ë”¥ ëŸ¬ë‹ìœ¼ë¡œ ë°œì „í•œë‹¤.
-   ë”¥ ëŸ¬ë‹ì€ Input nodes layer, Hidden nodes layer, Output nodes layer, ì´ë ‡ê²Œ ì„¸ ê°€ì§€ ì¸µì´ ì¡´ì¬í•œë‹¤.

## ëª©ì°¨

1. [Perceptron](#perceptron)
2. [Tensorflow](#tensorflow)
3. [CNN](#cnn)

---

## Perceptron

<div id="Perceptron">

### SLP (Single Layer Perceptron), ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ , ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ 

-   ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì˜ ì‹ ê²½ë§ìœ¼ë¡œì„œ, Hidden Layerê°€ ì—†ê³  Single Layerë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.
-   í¼ì…‰íŠ¸ë¡ ì˜ êµ¬ì¡°ëŠ” ì…ë ¥ featureì™€ ê°€ì¤‘ì¹˜, activation function, ì¶œë ¥ ê°’ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.
-   ì‹ ê²½ ì„¸í¬ì—ì„œ ì‹ í˜¸ë¥¼ ì „ë‹¬í•˜ëŠ” ì¶•ì‚­ëŒê¸°ì˜ ì—­í• ì„ í¼ì…‰íŠ¸ë¡ ì—ì„œëŠ” ê°€ì¤‘ì¹˜ê°€ ëŒ€ì‹ í•˜ê³ ,  
     ì…ë ¥ ê°’ê³¼ ê°€ì¤‘ì¹˜ ê°’ì€ ëª¨ë‘ ì¸ê³µ ë‰´ë ¨(í™œì„± í•¨ìˆ˜)ìœ¼ë¡œ ë„ì°©í•œë‹¤.
-   ê°€ì¤‘ì¹˜ì˜ ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ì…ë ¥ ê°’ì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ëœ»ì´ê³ , ì¸ê³µ ë‰´ëŸ°(í™œì„± í•¨ìˆ˜)ì— ë„ì°©í•œ ê° ì…ë ¥ ê°’ê³¼ ê°€ì¤‘ì¹˜ ê°’ì„ ê³±í•œ ë’¤ ì „ì²´ í•©í•œ ê°’ì„ êµ¬í•œë‹¤.
-   ì¸ê³µ ë‰´ëŸ°(í™œì„± í•¨ìˆ˜)ì€ ë³´í†µ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ ê°™ì€ ê³„ë‹¨ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬,  
     í•©í•œ ê°’ì„ í™•ë¥ ë¡œ ë³€í™˜í•˜ê³  ì´ ë•Œ, ì„ê³„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 0 ë˜ëŠ” 1ì„ ì¶œë ¥í•œë‹¤.

-   ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì´ ì¸ê³µ ì‹ ê²½ë§ì—ì„œëŠ” í•˜ë‚˜ì˜ ì¸ê³µ ë‰´ëŸ°ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.
-   ê²°ê³¼ì ìœ¼ë¡œ í¼ì…‰íŠ¸ë¡ ì˜ íšŒê·€ ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ ì°¨ì´ê°€ ìµœì†Œê°€ ë˜ëŠ” ê°€ì¤‘ì¹˜ ê°’ì„ ì°¾ëŠ” ê³¼ì •ì´ í¼ì…‰íŠ¸ë¡ ì´ í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ë‹¤.
-   ìµœì†Œ ê°€ì¤‘ì¹˜ ê°’ì„ ì„¤ì •í•œ ë’¤ ì…ë ¥ feature ê°’ìœ¼ë¡œ ì˜ˆì¸¡ ê°’ì„ ê³„ì‚°í•˜ê³ , ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´ë¥¼ êµ¬í•œ ë’¤ ì´ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë„ë¡ ê°€ì¤‘ì¹˜ ê°’ì„ ë³€ê²½í•œë‹¤.
-   í¼ì…‰íŠ¸ë¡ ì˜ í™œì •í™” ì •ë„ë¥¼ í¸í–¥(bias)ìœ¼ë¡œ ì¡°ì ˆí•  ìˆ˜ ìˆìœ¼ë©°, í¸í–¥ì„ í†µí•´ ì–´ëŠì •ë„ì˜ ìê·¹ì„ ë¯¸ë¦¬ ì£¼ê³  ì‹œì‘ í•  ìˆ˜ ìˆë‹¤.
-   ë‰´ëŸ°ì´ í™œì„±í™”ë˜ê¸° ìœ„í•´ í•„ìš”í•œ ìê·¹ì´ 1000ì´ë¼ê³  ê°€ì •í•˜ë©´, ì…ë ¥ ê°’ì„ 500ë§Œ ë°›ì•„ë„ í¸í–¥ì„ 2ë¡œ ì£¼ì–´ 1000ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

-   í¼ì…‰íŠ¸ë¡ ì˜ ì¶œë ¥ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ë°©í–¥ì„±ìœ¼ë¡œ ê³„ì†í•´ì„œ ê°€ì¤‘ì¹˜ ê°’ì„ ë³€ê²½í•˜ë©°, ì´ ë•Œ ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•œë‹¤.

#### SGD (Stochastiic Gradient Descent), í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•

-   ê²½ì‚¬ í•˜ê°•ë²• ë°©ì‹ì€ ì „ì²´ í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°í•œë‹¤. í•˜ì§€ë§Œ ì…ë ¥ ë°ì´í„°ê°€ í¬ê³  ë ˆì´ì–´ê°€ ë§ì„ ìˆ˜ë¡ ë§ì€ ìì›ì´ ì†Œëª¨ëœë‹¤.
-   ì¼ë°˜ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ì—°ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì—, ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ SGD ë°©ì‹ì´ ë„ì…ë˜ì—ˆë‹¤.
-   ì „ì²´ í•™ìŠµ ë°ì´í„° ì¤‘, ë‹¨ í•œ ê±´ë§Œ ì„ì˜ë¡œ ì„ íƒí•˜ì—¬ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‹¤ì‹œí•˜ëŠ” ë°©ì‹ì„ ì˜ë¯¸í•œë‹¤.
-   ë§ì€ ê±´ ìˆ˜ ì¤‘ì— í•œ ê±´ë§Œ ì‹¤ì‹œí•˜ê¸° ë•Œë¬¸ì—, ë¹ ë¥´ê²Œ ìµœì ì ì„ ì°¾ì„ ìˆ˜ ìˆì§€ë§Œ ë…¸ì´ì¦ˆê°€ ì‹¬í•˜ë‹¤.
-   ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œëœ ìƒ˜í”Œ ë°ì´í„°ì— ëŒ€í•´ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‹¤ì‹œí•˜ê¸° ë•Œë¬¸ì— ì§„í­ì´ í¬ê³  ë¶ˆì•ˆì •í•´ ë³´ì¼ ìˆ˜ ìˆë‹¤.
-   ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , SGDë¥¼ ì–˜ê¸°í•  ë•Œì—ëŠ” ë³´í†µ ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì˜ë¯¸í•œë‹¤.

-   ì „ì²´ í•™ìŠµ ë°ì´í„° ì¤‘, íŠ¹ì • í¬ê¸°(Batch í¬ê¸°)ë§Œí¼ ì„ì˜ë¡œ ì„ íƒí•´ì„œ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‹¤ì‹œí•œë‹¤. ì´ ë˜í•œ, í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•

-   ì „ì²´ í•™ìŠµ ë°ì´í„°ê°€ 1000ê±´ì´ë¼ê³  í•˜ê³ , batch sizeë¥¼ 100ê±´ì´ë¼ ê°€ì •í•˜ë©´, ì „ì²´ ë°ì´í„°ë¥¼ batch sizeë§Œí¼ ë‚˜ëˆ ì„œ ê°€ì ¸ì˜¨ ë’¤ ì„ê³ , ê²½ì‚¬í•˜ê°•ë²•ì„ ê³„ì‚°í•œë‹¤.  
    ì´ ê²½ìš°, 10ë²ˆ ë°˜ë³µí•´ì•¼ 1000ê°œì˜ ë°ì´í„°ê°€ ëª¨ë‘ í•™ìŠµë˜ê³  ì´ë¥¼ epochë¼ê³  í•œë‹¤. ì¦‰, 10 epoch \* 100 batch ì´ë‹¤.

### (MLP) Multi Layer Perceptron, ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ , ë‹¤ì¤‘ í¼ì…‰íŠ¸ë¡ 

-   ë³´ë‹¤ ë³µì¡í•œ ë¬¸ì œì˜ í•´ê²°ì„ ìœ„í•´ì„œ ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µ ì‚¬ì´ì— ì€ë‹‰ì¸µì´ í¬í•¨ë˜ì–´ ìˆë‹¤.
-   í¼ì…‰íŠ¸ë¡ ì„ ì—¬ëŸ¬ì¸µ ìŒ“ì€ ì¸ê³µ ì‹ ê²½ë§ìœ¼ë¡œì„œ, ê° ì¸µì—ëŠ” í™œì„±í•¨ìˆ˜ë¥¼ í†µí•´ ì…ë ¥ì„ ì²˜ë¦¬í•œë‹¤.
-   ì¸µì´ ê¹Šì–´ì§ˆ ìˆ˜ë¡ ì •í™•í•œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•´ì§€ì§€ë§Œ, ë„ˆë¬´ ê¹Šì–´ì§€ë©´ Overfittingì´ ë°œìƒí•œë‹¤.

#### ANN (Artificial Neural Network), ì¸ê³µ ì‹ ê²½ë§

-   ì€ë‹‰ì¸µì´ 1ê°œì¼ ê²½ìš° ì´ë¥¼ ì¸ê³µ ì‹ ê²½ë§ì´ë¼ê³  í•œë‹¤.

#### DNN (Deep Neural Network), ì‹¬ì¸µ ì‹ ê²½ë§

-   ì€ë‹‰ì¸µì´ 2ê°œ ì´ìƒì¼ ê²½ìš° ì´ë¥¼ ì‹¬ì¸µ ì‹ ê²½ë§ì´ë¼ê³  í•œë‹¤.

#### **Back-propagation, ì—­ì „íŒŒ**

-   ì‹¬ì¸µ ì‹ ê²½ë§ì—ì„œ ìµœì¢… ì¶œë ¥(ì˜ˆì¸¡)ì„ í•˜ê¸° ìœ„í•œ ì‹ì´ ìƒê¸°ì§€ë§Œ ì‹ì´ ë„ˆë¬´ ë³µì¡í•´ì§€ê¸° ë•Œë¬¸ì— í¸ë¯¸ë¶„ì„ ì§„í–‰í•˜ê¸°ì— í•œê³„ê°€ ìˆë‹¤.
-   ì¦‰, í¸ë¯¸ë¶„ì„ í†µí•´ ê°€ì¤‘ì¹˜ ê°’ì„ êµ¬í•˜ê³ , ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•´ ê°€ì¤‘ì¹˜ ê°’ì„ ì—…ë°ì´íŠ¸ í•˜ë©°, ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ì•„ì•¼ í•˜ëŠ”ë°, ìˆœë°©í–¥ìœ¼ë¡œëŠ” ë³µì¡í•œ ë¯¸ë¶„ì‹ì„ ê³„ì‚°í•  ìˆ˜ê°€ ì—†ë‹¤.  
    ë”°ë¼ì„œ ë¯¸ë¶„ì˜ ì—°ì‡„ ë²•ì¹™(Chain Rule)ì„ ì‚¬ìš©í•˜ì—¬ ì—­ë°©í–¥ìœ¼ë¡œ í¸ë¯¸ë¶„ì„ ì§„í–‰í•œë‹¤.

### Activation Function, í™œì„±í™” í•¨ìˆ˜

-   ì¸ê³µ ì‹ ê²½ë§ì—ì„œ ì…ë ¥ ê°’ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•œ ë’¤ í•©í•œ ê²°ê³¼ë¥¼ ì ìš©í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.

---

1. **ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜**
    - ì€ë‹‰ì¸µì´ ì•„ë‹Œ ìµœì¢… í™œì„±í™” í•¨ìˆ˜, ì¶œë ¥ì¸µì—ì„œ ì‚¬ìš©ëœë‹¤.
    - ì€ë‹‰ì¸µì—ì„œ ì‚¬ìš© ì‹œ, ì…ë ¥ ê°’ì´ ì–‘ì˜ ë°©í–¥ìœ¼ë¡œ í° ê°’ì¼ ê²½ìš° ì¶œë ¥ê°’ì˜ ë³€í™”ê°€ ì—†ìœ¼ë©°, ìŒì˜ ë°©í–¥ë„ ë§ˆì°¬ê°€ì§€ì´ë‹¤.  
      í‰ê· ì´ 0ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì •ê·œ ë¶„í¬ í˜•íƒœê°€ ì•„ë‹ˆê³ , ì´ëŠ” ë°©í–¥ì— ë”°ë¼ ê¸°ìš¸ê¸°ê°€ ë‹¬ë ¤ì ¸ì„œ íƒìƒ‰ ê²½ë¡œê°€ ë¹„íš¨ìœ¨ì (ì§€ê·¸ì¬ê·¸)ì´ ëœë‹¤.
2. **ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜**

    - ì€ë‹‰ì¸µì´ ì•„ë‹Œ ìµœì¢… í™œì„±í™” í•¨ìˆ˜(ì¶œë ¥ì¸µ)ì—ì„œ ì‚¬ìš©ëœë‹¤.
    - ì‹œê·¸ëª¨ì´ë“œì™€ ìœ ì‚¬í•˜ê²Œ 0 ~ 1 ì‚¬ì´ì˜ ê°’ì„ ì¶œë ¥í•˜ì§€ë§Œ, ì´ì§„ ë¶„ë¥˜ê°€ ì•„ë‹Œ **ë‹¤ì¤‘ ë¶„ë¥˜**ë¥¼ í†µí•´ ëª¨ë“  í™•ë¥ ê°’ì´ 1ì´ ë˜ë„ë¡ í•´ì¤€ë‹¤.
    - ì—¬ëŸ¬ ê°œì˜ íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë‹¤ì¤‘ ë¶„ë¥˜ì˜ ìµœì¢… í™œì„±í™” í•¨ìˆ˜(ì¶œë ¥ì¸µ)ë¡œ ì‚¬ìš©ëœë‹¤.

3. íƒ„ì  íŠ¸ í•¨ìˆ˜

    - ì€ë‹‰ì¸µì´ ì•„ë‹Œ ìµœì¢… í™œì„±í™” í•¨ìˆ˜(ì¶œë ¥ì¸µ)ì—ì„œ ì‚¬ìš©ëœë‹¤.
    - ì€ë‹‰ì¸µì—ì„œ ì‚¬ìš© ì‹œ, ì‹œê·¸ëª¨ì´ë“œì™€ ë‹¬ë¦¬ -1 ~ 1 ì‚¬ì´ì˜ ê°’ì´ ì¶œë ¥í•´ì„œ í‰ê· ì´ 0ì´ ë  ìˆ˜ ìˆì§€ë§Œ,  
      ì—¬ì „íˆ ì…ë ¥ ê°’ì˜ ì–‘ì˜ ë°¥í–¥ìœ¼ë¡œ í° ê°’ì¼ ê²½ìš° ì¶œë ¥ê°’ì˜ ë³€í™”ê°€ ë¯¸ë¹„í•˜ê³  ìŒì˜ ë°©í–¥ë„ ë§ˆì°¬ê°€ì§€ ì´ë‹¤.

4. **ë ë£¨ í•¨ìˆ˜**
    - ëŒ€í‘œì ì¸ ì€ë‹‰ì¸µì˜ í™œì„± í•¨ìˆ˜ì´ë‹¤.
    - ì…ë ¥ ê°’ì´ 0ë³´ë‹¤ ì‘ìœ¼ë©´ ì¶œë ¥ì€ 0, 0ë³´ë‹¤ í¬ë©´ ì…ë ¥ê°’ì„ ì¶œë ¥í•˜ê²Œ ëœë‹¤.

### Optimizer, ìµœì í™”

-   ìµœì ì˜ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•˜ë©°, ìµœì†Œê°’ì„ ì°¾ì•„ê°€ëŠ” ë°©ë²•ë“¤ì„ ì˜ë¯¸í•œë‹¤.
-   lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì†Œ lossë¥¼ ë³´ë‹¤ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

#### Momentum

-   ê°€ì¤‘ì¹˜ë¥¼ ê³„ì† ì—…ë°ì´íŠ¸í•  ë•Œë§ˆë‹¤ ì´ì „ì˜ ê°’ì„ ì¼ì • ìˆ˜ì¤€ ë°˜ì˜ì‹œí‚¤ë©´ì„œ ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¡œ ì—…ë°ì´íŠ¸í•œë‹¤.
-   ì§€ì—­ ìµœì†Œê°’ì—ì„œ ë²—ì–´ë‚˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©°, ì§„í–‰í–ˆë˜ ë°©í–¥ë§Œí¼ ì¶”ê°€ì ìœ¼ë¡œ ë”í•˜ì—¬, ê´€ì„±ì²˜ëŸ¼ ë¹ ì ¸ë‚˜ì˜¬ ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.

#### AdaGrad (Adaptive Gradient)

-   ê°€ì¤‘ì¹˜ ë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµë¥ ì„ ë™ì ìœ¼ë¡œ ì ìš©í•œë‹¤.
-   ì ê²Œ ë³€í™”ëœ ê°€ì¤‘ì¹˜ëŠ” ë³´ë‹¤ í° í•™ìŠµë¥ ì„ ì ìš©í•˜ê³ , ë§ì´ ë³€í™”ëœ ê°€ì¤‘ì¹˜ëŠ” ë³´ë‹¤ ì‘ì€ í•™ìŠµë¥ ì„ ì ìš©ì‹œí‚¨ë‹¤.
-   ì²˜ìŒì—ëŠ” í° ë³´í­ìœ¼ë¡œ ì´ë™í•˜ë‹¤ê°€ ìµœì†Œê°’ì— ê°€ê¹Œì›Œì§ˆ ìˆ˜ë¡ ì‘ì€ ë³´í­ìœ¼ë¡œ ì´ë™í•˜ê²Œ ëœë‹¤.
-   ê³¼ê±°ì˜ ëª¨ë“  ê¸°ìš¸ê¸°ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµë¥ ì´ ê¸‰ê²©íˆ ê°ì†Œí•˜ì—¬, ë¶„ëª¨ê°€ ì»¤ì§ìœ¼ë¡œì¨ í•™ìŠµë¥ ì´ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ë¬¸ì œê°€ ìˆë‹¤.

#### RMSProp (Root Mean Sqaure Propagation)

-   AdaGradì˜ ë‹¨ì ì„ ë³´ì™„í•œ ê¸°ë²•ìœ¼ë¡œì„œ, í•™ìŠµë¥ ì´ ì§€ë‚˜ì¹˜ê²Œ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ ì§€ìˆ˜ ê°€ì¤‘ í‰ê· ë²•(exponentially weighted average)ì„ í†µí•´ êµ¬í•œë‹¤.
-   ì§€ìˆ˜ ê°€ì¤‘ í‰ê· ë²•ì´ë€, ë°ì´í„°ì˜ ì´ë™ í‰ê· ì„ êµ¬í•  ë•Œ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì‡ í•˜ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤.
-   ì´ì „ì˜ ê¸°ìš¸ê¸°ë“¤ì„ ë˜‘ê°™ì´ ë”í•´ê°€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í›¨ì”¬ ì´ì „ì˜ ê¸°ìš¸ê¸°ëŠ” ì¡°ê¸ˆ ë°˜ì˜í•˜ê³  ìµœê·¼ì˜ ê¸°ìš¸ê¸°ë¥¼ ë§ì´ ë°˜ì˜í•œë‹¤.
-   featureë§ˆë‹¤ ì ì ˆí•œ í•™ìŠµë¥ ì„ ì ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆê³ , AdaGradë³´ë‹¤ í•™ìŠµì„ ì˜¤ë˜ í•  ìˆ˜ ìˆë‹¤.

#### Adam (Adaptive Moment Estimation)

-   Momentumê³¼ RMSProp ë‘ ê°€ì§€ ë°©ì‹ì„ ê²°í•©í•œ í˜•íƒœë¡œì„œ, ì§„í–‰í•˜ë˜ ì†ë„ì— ê´€ì„±ì„ ì£¼ê³ , ì§€ìˆ˜ ê°€ì¤‘ í‰ê· ë²•ì„ ì ìš©í•œ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.
-   ìµœì í™” ë°©ë²• ì¤‘ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë©°, ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

</div>

## tensorflow

<div id="tensorflow">

### Tensorflow, í…ì„œí”Œë¡œìš°

-   êµ¬ê¸€ì´ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ì†Œí”„íŠ¸ì›¨ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë©°, ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.
-   ì£¼ë¡œ ì´ë¯¸ì§€ ì¸ì‹ì´ë‚˜ ë°˜ë³µ ì‹ ê²½ë§ êµ¬ì„±, ê¸°ê³„ ë²ˆì—­, í•„ê¸° ìˆ«ì íŒë³„ ë“±ì„ ìœ„í•œ ê°ì¢… ì‹ ê²½ë§ í•™ìŠµì— ì‚¬ìš©ëœë‹¤.
-   ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ, ê¸°ì´ˆë¶€í„° ì„¸ì„¸í•˜ê²Œ ì‘ì—…í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì§„ì…ì¥ë²½ì´ ë†’ë‹¤.

### Keras, ì¼€ë¼ìŠ¤

-   ì¼ë°˜ ì‚¬ìš© ì‚¬ë¡€ì— ìµœì í™”ë˜ê³  "ìµœì í™”, ê°„ë‹¨, ì¼ê´€, ë‹¨ìˆœí™”"ëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•œë‹¤.
-   ì†ì‰½ê²Œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ê°œë°œí•˜ê³  í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì§ê´€ì ì¸ APIë¥¼ ì œê³µí•œë‹¤.
-   í…ì„œí”Œë¡œìš° 2ë²„ì „ ì´ìƒë¶€í„° ì¼€ë¼ìŠ¤ê°€ í¬í•¨ë˜ì—ˆê¸° ë•Œë¬¸ì— í…ì„œí”Œë¡œìš°ë¥¼ í†µí•´ ì¼€ë¼ìŠ¤ë¥¼ ì‚¬ìš©í•œë‹¤.
-   ê¸°ì¡´ Keras íŒ¨í‚¤ì§€ë³´ë‹¤ëŠ” ì´ì œ Tensorflowì— ë‚´ì¥ëœ Keras ì‚¬ìš©ì´ ë” ê¶Œì¥ëœë‹¤.

---

#### Sequential API

-   ê°„ë‹¨í•œ ëª¨ë¸ì„ êµ¬í˜„í•˜ê¸°ì— ì í•©í•˜ê³  ë‹¨ìˆœí•˜ê²Œ ì¸µì„ ìŒ“ëŠ” ë°©ì‹ìœ¼ë¡œ ì‰½ê³  ì‚¬ìš©í•˜ê¸°ê°€ ê°„ë‹¨í•˜ë‹¤.
-   ë‹¨ì¼ ì…ë ¥ ë° ì¶œë ¥ë§Œ ìˆìœ¼ë¯€ë¡œ ë ˆì´ì–´ë¥¼ ê³µìœ í•˜ê±°ë‚˜ ì—¬ëŸ¬ ì…ë ¥ ë˜ëŠ” ì¶œë ¥ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ì—†ë‹¤.

#### Funcional API

-   Funtional APIëŠ” Sequential APIë¡œëŠ” êµ¬í˜„í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•œ ëª¨ë¸ë“¤ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
-   ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ ë° ì¶œë ¥ì„ ê°€ì§„ ëª¨ë¸ì„ êµ¬í˜„í•˜ê±°ë‚˜ ì¸µ ê°„ì˜ ì—°ê²° ë° ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ êµ¬í˜„ ì‹œ ì‚¬ìš©í•œë‹¤.

---

### Grayscale, RGB

-   í‘ë°± ì´ë¯¸ì§€ì™€ ì»¬ëŸ¬ ì´ë¯¸ì§€ëŠ” ê° 2ì°¨ì›ê³¼ 3ì°¨ì›ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.
-   í‘ë°± ì´ë¯¸ì§€ëŠ” 0 ~ 255ë¥¼ ê°–ëŠ” 2ì°¨ì› ë°°ì—´(ë†’ì´ X ë„ˆë¹„)ì´ê³ ,  
    ì»¬ëŸ¬ ì´ë¯¸ì§€ëŠ” 0 ~ 255ë¥¼ ê°–ëŠ” R, G, B 2ì°¨ì› ë°°ì—´ 3ê°œë¥¼ ê°–ëŠ” 3ì°¨ì›(ë†’ì´ X ë„ˆë¹„ X ì±„ë„)ì´ë‹¤.

### Grayscale Image Matrix

-   ê²€ì€ìƒ‰ì— ê°€ê¹Œìš´ ìƒ‰ì€ 0ì— ê°€ê¹ê³  í°ìƒ‰ì— ê°€ê¹Œìš°ë©´ 255ì— ê°€ê¹ë‹¤.
-   ëª¨ë“  í”½ì…€ì´ featureì´ë‹¤.

---

### Callback API (í™œìš©ì„±ì´ ë†’ìŒ!)

-   ëª¨ë¸ì´ í•™ìŠµ ì¤‘ì— ì¶©ëŒì´ ë°œìƒí•˜ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ê°€ ëŠê¸°ë©´, ëª¨ë“  í›ˆë ¨ ì‹œê°„ì´ ë‚­ë¹„ë  ìˆ˜ ìˆê³ ,  
    ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ í›ˆë ¨ì„ ì¤‘ê°„ì— ì¤‘ì§€í•´ì•¼ í•  ìˆ˜ë„ ìˆë‹¤.
-   ëª¨ë¸ì´ í•™ìŠµì„ ì‹œì‘í•˜ë©´ í•™ìŠµì´ ì™„ë£Œë  ë•Œê¹Œì§€ ì•„ë¬´ëŸ° ì œì–´ë¥¼ í•˜ì§€ ëª»í•˜ê²Œ ë˜ê³ ,  
    ì‹ ê²½ë§ í›ˆë ¨ì„ ì™„ë£Œí•˜ëŠ” ë°ì—ëŠ” ëª‡ ì‹œê°„ ë˜ëŠ” ë©°ì¹ ì´ ê±¸ë¦´ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì œì–´í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ í•„ìš”í•˜ë‹¤.
-   í›ˆë ¨ ì‹œ(fit()) Callback APIë¥¼ ë“±ë¡ì‹œí‚¤ë©´ ë°˜ë³µ ë‚´ì—ì„œ íŠ¹ì • ì´ë²¤íŠ¸ ë°œìƒë§ˆë‹¤ ë“±ë¡ëœ callbackì´ í˜¸ì¶œë˜ì–´ ìˆ˜í–‰ëœë‹¤.

**1) ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weight_only=False, mode='auto')**

-   íŠ¹ì • ì¡°ê±´ì— ë”°ë¼ì„œ ëª¨ë¸ ë˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•œë‹¤.
-   filepath: "weight.{epoch: 03d}-{val_loss:.4f}-{acc:.4f}.weights.hdf5" ì™€ ê°™ì´ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•œë‹¤.
-   monitor: ëª¨ë‹ˆí„°ë§í•  ì„±ëŠ¥ ì§€í‘œë¥¼ ì‘ì„±í•œë‹¤.
-   save_best_only: ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ì„ ì €ì¥í•  ì§€ì— ëŒ€í•œ ì—¬ë¶€
-   save_weights_only: weightsë§Œ ì €ì¥í•  ì§€ì— ëŒ€í•œ ì—¬ë¶€
-   mode: {auto, min, max} ì¤‘ í•œ ê°€ì§€ë¥¼ ì‘ì„±í•œë‹¤. monitorì˜ ì„±ëŠ¥ ì§€í‘œì— ë”°ë¼ ì¢‹ì€ ê²½ìš°ë¥¼ ì„ íƒí•œë‹¤.  
    \*monitorì˜ ì„±ëŠ¥ ì§€í‘œê°€ ê°ì†Œí•´ì•¼ ì¢‹ì€ ê²½ìš° min, ì¦ê°€í•´ì•¼ ì¢‹ì€ ê²½ìš° max, monitorì˜ ì´ë¦„ìœ¼ë¡œë¶€í„° ìë™ìœ¼ë¡œ ìœ ì¶”í•˜ê³  ì‹¶ë‹¤ë©´ autoë¥¼ ì‚¬ìš©í•œë‹¤.

**2) ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto, min_lr=0')** (LR: Learning Rate)

-   íŠ¹ì • ë°˜ë³µë™ì•ˆ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ, í•™ìŠµë¥ ì„ ë™ì ìœ¼ë¡œ ê°ì†Œì‹œí‚¨ë‹¤.
-   monitor: ëª¨ë‹ˆí„°ë§í•  ì„±ëŠ¥ ì§€í‘œë¥¼ ì‘ì„±í•œë‹¤.
-   factor: í•™ìŠµë¥ ì„ ê°ì†Œì‹œí‚¬ ë¹„ìœ¨, ìƒˆë¡œìš´ í•™ìŠµë¥  = ê¸°ì¡´ í•™ìŠµë¥  \* factor
-   patience: í•™ìŠµë¥ ì„ ì¤„ì´ê¸° ì „ì— monitorí•  ë°˜ë³µ íšŸìˆ˜
-   mode: {auto, min, max} ì¤‘ í•œ ê°€ì§€ë¥¼ ì‘ì„±í•œë‹¤. monitorì˜ ì„±ëŠ¥ ì§€í‘œì— ë”°ë¼ ì¢‹ì€ ê²½ìš°ë¥¼ ì„ íƒí•œë‹¤.  
    \*monitorì˜ ì„±ëŠ¥ ì§€í‘œê°€ ê°ì†Œí•´ì•¼ ì¢‹ì€ ê²½ìš° min, ì¦ê°€í•´ì•¼ ì¢‹ì€ ê²½ìš° max, monitorì˜ ì´ë¦„ìœ¼ë¡œë¶€í„° ìë™ìœ¼ë¡œ ìœ ì¶”í•˜ê³  ì‹¶ë‹¤ë©´ autoë¥¼ ì‚¬ìš©í•œë‹¤.

**3) EarlyStopping(monitor='val_loss'm patient=0, verbose=0, mode='auto')**

-   íŠ¹ì • ë°˜ë³µë™ì•ˆ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ, í•™ìŠµì„ ì¡°ê¸°ì— ì¤‘ë‹¨í•œë‹¤.
-   monitor: ëª¨ë‹ˆí„°ë§í•  ì„±ëŠ¥ ì§€í‘œë¥¼ ì‘ì„±í•œë‹¤.
-   patience: Early Stoppingì„ ì ìš©í•˜ê¸° ì „ì— monitorí•  ë°˜ë³µ íšŸìˆ˜.
-   mode: {auto, min, max} ì¤‘ í•œ ê°€ì§€ë¥¼ ì‘ì„±í•œë‹¤. monitorì˜ ì„±ëŠ¥ ì§€í‘œì— ë”°ë¼ ì¢‹ì€ ê²½ìš°ë¥¼ ì„ íƒí•œë‹¤.  
    \*monitorì˜ ì„±ëŠ¥ ì§€í‘œê°€ ê°ì†Œí•´ì•¼ ì¢‹ì€ ê²½ìš° min, ì¦ê°€í•´ì•¼ ì¢‹ì€ ê²½ìš° max, monitorì˜ ì´ë¦„ìœ¼ë¡œë¶€í„° ìë™ìœ¼ë¡œ ìœ ì¶”í•˜ê³  ì‹¶ë‹¤ë©´ autoë¥¼ ì‚¬ìš©í•œë‹¤.

#### <div id="tensowflow-code">tensorflow Code</div>

<details>
    <summary> 1. kerasì—ì„œ ë¶ˆëŸ¬ì˜¨ ì´ë¯¸ì§€ì— ëŒ€í•˜ì—¬ ì´ë¯¸ì§€ í‘œê¸° í•¨ìˆ˜ ì½”ë“œ</summary>

        def show_images(images, targets, ncols=8):
        figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)
        for i in range(ncols):
            axs[i].imshow(images[i], cmap='gray')
            axs[i].set_title(class_names[targets[i]])

        show_images(train_images[:8], train_targets[:8])
        show_images(train_images[8:16], train_targets[8:16])

</details>

<details>
    <summary> 2. Sequential API Code</summary>

        from tensorflow.keras.layers import Dense, Flatten
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.losses import CategoricalCrossentropy
        from tensorflow.keras.optimizers import Adam

        # shape
        INPUT_SIZE = 28

        model = Sequential([
            # ì…ë ¥ì¸µ
            Flatten(input_shape=(INPUT_SIZE, INPUT_SIZE)),
            # ì€ë‹‰ì¸µ
            Dense(64, activation='relu'),
            # ì€ë‹‰ì¸µ
            Dense(128, activation='relu'),
            # ì¶œë ¥ì¸µ (ë‹¤ì¤‘ ë¶„ë¥˜ ì´ê¸° ë•Œë¬¸ì— í™œì„± í•¨ìˆ˜ëŠ” softmax ì‚¬ìš©)
            Dense(10, activation='softmax')
        ])

        # # ê²½ì‚¬í•˜ê°•ë²• optimizer, ë° ìµœì í™”
        # ì†ì‹¤í•¨ìˆ˜ëŠ” ë‹¤ì¤‘ í•¨ìˆ˜ì´ê¸° ë•Œë¬¸ì— CategoricalCrossentropy ì‚¬ìš©
        model.compile(optimizer=Adam(0.001), loss=CategoricalCrossentropy(), metrics=['acc'])

</details>

<details>
    <summary>3. ê²€ì¦ë°ì´í„°ë¥¼ í¬í•¨í•œ ì •í™•ë„ ê·¸ë˜í”„ í‘œí˜„</summary>
        #ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ
        import matplotlib.pyplot as plt

        plt.plot(history.history['acc'], label='train')
        plt.plot(history.history['val_acc'], label='validation')
        plt.legend()
        plt.show()

</details>

<details>
<summary>4. ê²€ì¦ ë°ì´í„° ì •í™•ë„ ë° ì†ì‹¤ í•¨ìˆ˜ í™•ì¸</summary>

    # ê²€ì¦ë°ì´í„°ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ evaluate í•¨ìˆ˜ ì‚¬ìš©
    model.evaluate(test_images, test_oh_targets, batch_size=32)

</details>

<details>
    <summary> 5. Funtional API Code</summary>

        ### call ë§¤ì§ ë©”ì†Œë“œ
        # call í•¨ìˆ˜ (ë§¤ì§ ë©”ì†Œë“œ)
        class Test:
            def __call__(self,data):
                return data + 10

---

        # call í•¨ìˆ˜ ë•ë¶„ì— ìƒì„±ì ë’¤ì— ê°’ì„ ë„£ì–´ì¤˜ì„œ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.

        from tensorflow.keras.layers import Layer, Input, Dense, Flatten
        from tensorflow.keras.models import Model

        INPUT_SIZE = 28

        def create_model():
            input_tensor = Input((INPUT_SIZE,INPUT_SIZE))
            x = Flatten()(input_tensor)
            x = Dense(64, activation='relu')(x)
            x = Dense(128, activation='relu')(x)
            output = Dense(10, activation='softmax')(x)

            model = Model(inputs=input_tensor, outputs=output)
            return model


        model = create_model()
        model.summary()

</details>

<details>
    <summary>6. tensorflow ì „ì²˜ë¦¬ ê³¼ì • (arrayê°ì²´ ë³€í™˜, ì›-í•« ì¸ì½”ë”©, í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬)</summary>

    from tensorflow.keras.utils import to_categorical
    from sklearn.model_selection import train_test_split
    import numpy as np

    (train_images, train_targets), (test_images, test_targets) = fashion_mnist.load_data()

    # array ê°ì²´ ë³€í™˜ ë° ì‹¤ìˆ˜ ë³€í™˜, ìƒ‰ìƒì„ í‘œí˜„í•˜ê¸° ìœ„í•´ 255.0 ìœ¼ë¡œ ë³€í™˜
    def get_preprocessed_data(images, targets):
        images = np.array(images / 255.0, dtype=np.float32)
        targets = np.array(targets, dtype=np.float32)

        return images, targets

    # íƒ€ê²Ÿ ë°ì´í„°ì— ëŒ€í•˜ì—¬ ì›-í•« ì¸ì½”ë”© ë©”ì†Œë“œ ìƒì„±
    def get_preprocessed_ohe(images, targets):
        images, targets = get_preprocessed_data(images, targets)
        oh_targets = to_categorical(targets)

        return images, oh_targets

    # í›ˆë ¨, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬í•˜ê¸° ìœ„í•œ ë©”ì†Œë“œ ìƒì„±
    def get_train_valid_test(train_images, train_targets, test_images, test_targets, validation_size=0.2, random_state=124):
        train_images, train_oh_targets = get_preprocessed_ohe(train_images, train_targets)
        test_images, test_oh_targets = get_preprocessed_ohe(test_images, test_targets)

        train_images, validation_images, train_oh_targets, validation_oh_targets = \
        train_test_split(train_images, train_oh_targets, stratify=train_oh_targets, test_size=validation_size, random_state=random_state)

        return (train_images, train_oh_targets), (validation_images, validation_oh_targets), (test_images, test_oh_targets)

---

    from tensorflow.keras.datasets import fashion_mnist

    (train_images, train_targets), (test_images, test_targets) = fashion_mnist.load_data()

    (train_images, train_oh_targets), (validation_images, validation_oh_targets), (test_images, test_oh_targets) = \
    get_train_valid_test(train_images, train_targets, test_images, test_targets)

    print(train_images.shape, train_oh_targets.shape)
    print(validation_images.shape, validation_oh_targets.shape)
    print(test_images.shape, test_oh_targets.shape)

</details>

<details>
    <summary>tip. model.predict (pred_prob), np.argmax</summary>

        # í›ˆë ¨ê³¼ ì •ë‹µì˜ ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•´ ì°¨ì›ì„ ëŠ˜ë¦¬ëŠ” ì‘ì—…
        import numpy as np
        np.expand_dims(test_images[0], axis=0).shape

        # ì •ë‹µì´ ë‚˜ì˜¬ í™•ë¥ 
        pred_prob = model.predict(np.expand_dims(test_images[8500], axis=0))
        print(pred_prob)

        # ì •ë‹µì´ ë‚˜ì˜¬ í™•ë¥  ë° ì •ë‹´ì„ ì¶œë ¥
        pred_proba = model.predict(np.expand_dims(test_images[326], axis=0))
        print('softmax output:', pred_proba)

        # argmax() : ê°€ì¥ ë†’ì€ ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ì„œ í‘œê¸°í•˜ëŠ” í•¨ìˆ˜
        pred = np.argmax(np.squeeze(pred_proba))
        print('predicted target value:', pred)

</details>

<details>
    <summary>7. Callback API(ModelCheckpoint, ReduceLROnPlateau, EarlyStopping)</summary>

        from tensorflow.keras.optimizers import Adam
        from tensorflow.keras.losses import CategoricalCrossentropy
        from tensorflow.keras.callbacks import ModelCheckpoint

        model = create_model()
        model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])

        mcp_cb = ModelCheckpoint(
            filepath="./callback_files/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5",
            monitor='val_loss',
            save_best_only=False,
            # Modelì´ ì•„ëŠ” weight ë¥¼ ì €ì¥í•  ë•Œ Trueì„¤ì •
            save_weights_only=True,
            mode='min'
        )

        rlr_cb = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.1,
            patience=2,
            mode='min'
        )

        ely_cb = EarlyStopping(
            monitor='val_loss',
            patience=3,
            mode='min'
        )

        history = model.fit(x=train_images, y=train_oh_targets, validation_data=(validation_images, validation_oh_targets), batch_size=64, epochs=20, callbacks=[mcp_cb, rlr_cb, ely_cb])

</details>

<details>
    <summary>8. callbackì„ ì´ìš©í•œ ê°€ì¤‘ì¹˜ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°</summary>
        model.load_weights('./callback_files/')
        model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])
</details>

</div>

<hr>

## CNN

<div id="cnn">

### CNN (Convolutional Neural Network), í•©ì„±ê³± ì‹ ê²½ë§

-   ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ëŠ” ë¶„ë¥˜ ëŒ€ìƒì´ ì´ë¯¸ì§€ì—ì„œ ê³ ì •ëœ ìœ„ì¹˜ì— ìˆì§€ ì•Šì€ ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì´ë‹¤.
-   ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ì„œëŠ”, ì´ë¯¸ì§€ì˜ ê° featureë“¤ì„ ê·¸ëŒ€ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, CNNìœ¼ë¡œ íŒ¨í„´ì„ ì¸ì‹í•œ ë’¤ í•™ìŠµí•´ì•¼ í•œë‹¤.

-   ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ì»¤ì§ˆ ìˆ˜ë¡ êµ‰ì¥íˆ ë§ì€ Weightê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ë¶„ë¥˜ê¸°ì— ë°”ë¡œ ë„£ì§€ ì•Šê³ , ì´ë¥¼ ì‚¬ì „ì— ì¶”ì¶œ ë° ì¶•ì†Œí•´ì•¼ í•œë‹¤.
-   CNNì€ ì¸ê°„ì˜ ì‹œì‹ ê²½ êµ¬ì¡°ë¥¼ ëª¨ë°©í•œ ê¸°ìˆ ë¡œì„œ, ì´ë¯¸ì§€ì˜ íŒ¨í„´ì„ ì°¾ì„ ë•Œ ì‚¬ìš©í•œë‹¤.
-   Feature Extractionì„ í†µí•´ ê° ë‹¨ê³„ë¥¼ ê±°ì¹˜ë©´ì„œ, í•¨ì¶•ëœ ì´ë¯¸ì§€ ì¡°ê°ìœ¼ë¡œ ë¶„ë¦¬ë˜ê³  ê° ì´ë¯¸ì§€ ì¡°ê°ì„ í†µí•´ ì´ë¯¸ì§€ì˜ íŒ¨í„´ì„ ì¸ì‹í•œë‹¤.

-   CNNì€ ë¶„ë¥˜í•˜ê¸°ì— ì í•©í•œ ìµœì ì˜ featureë¥¼ ì¶”ì¶œí•˜ê³ , ìµœì ì˜ featureë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ìµœì ì˜ Weightì™€ filterë¥¼ ê³„ì‚°í•œë‹¤.

#### Filter

-   ì¼ë°˜ì ìœ¼ë¡œ ì •ë°© í–‰ë ¬ë¡œ êµ¬í˜„ë˜ì–´ ìˆê³ , ì›ë³¸ ì´ë¯¸ì§€ì— ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ìƒˆë¡œìš´ í”½ì…€ê°’ì„ ë§Œë“¤ë©´ì„œ ì ìš©í•œë‹¤.
-   ì‚¬ìš©ìê°€ ëª©ì ì— ë§ëŠ” íŠ¹ì • í•„í„°ë¥¼ ë§Œë“¤ê±°ë‚˜ ê¸°ì¡´ì— ì„¤ê³„ëœ ë‹¤ì–‘í•œ í•„í„°ë¥¼ ì„ íƒí•˜ì—¬ ì´ë¯¸ì§€ì— ì ìš©í•œë‹¤.  
    í•˜ì§€ë§Œ, CNNì€ ìµœì ì˜ í•„í„°ê°’ì„ í•™ìŠµí•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ìµœì í™” í•œë‹¤.

#### Kernel

-   filter ì•ˆì— 1 ~ nê°œì˜ ì»¤ë„ì´ ì¡´ì¬í•œë‹¤. ì»¤ë„ì˜ ê°œìˆ˜ëŠ” ë°˜ë“œì‹œ ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜ì™€ ë™ì¼í•´ì•¼ í•œë‹¤.
-   kernel SizeëŠ” ê°€ë¡œ X ì„¸ë¡œë¥¼ ì˜ë¯¸í•˜ë©°, ê°€ë¡œì™€ ì„¸ë¡œëŠ” ì„œë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ ë³´í†µì€ ì¼ì¹˜ì‹œí‚¨ë‹¤.
-   kernel Sizeê°€ í¬ë©´ í´ ìˆ˜ë¡ ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ ë” ë§ì€ feature ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì§€ë§Œ, í° ì‚¬ì´ì¦ˆì˜ kernelë¡œ Convolution Backboneì„ í•  ê²½ìš° í›¨ì”¬ ë” ë§ì€ ì—°ì‚°ëŸ‰ê³¼ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•˜ë‹¤.

\*\* ì»¤ë„, ì±„ë„, í•„í„°

#### Stride

-   ì…ë ¥ ì´ë¯¸ì§€ì— Convolution Filterë¥¼ ì ìš©í•  ë•Œ Slide Windowê°€ ì´ë™í•˜ëŠ” ê°„ê²©ì„ ì˜ë¯¸í•œë‹¤.
-   ê¸°ë³¸ strideëŠ” 1ì´ì§€ë§Œ, 2ë¥¼ ì ìš©í•˜ë©´ ì…ë ¥ feature map ëŒ€ë¹„ ì¶œë ¥ feature mapì˜ í¬ê¸°ê°€ ì ˆë°˜ì •ë„ ì¤„ì–´ë“ ë‹¤.
-   strideë¥¼ í‚¤ìš°ë©´ feature ì •ë³´ë¥¼ ì†ì‹¤í•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ì§€ë§Œ, ì˜¤íˆë ¤ ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì„ ì œê±°í•˜ëŠ” íš¨ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆê³  Convolution ì—°ì‚° ì†ë„ë¥¼ í–¥ìƒ ì‹œí‚¨ë‹¤.

#### Padding

-   Filterë¥¼ ì ìš©í•˜ì—¬ Convolution ìˆ˜í–‰ ì‹œ ì¶œë ¥ feature mapì´ ì…ë ¥ feature map ëŒ€ë¹„ ê³„ì†í•´ì„œ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.
-   Filter ì ìš© ì „, ì…ë ¥ feature mapì˜ ìƒí•˜ì¢Œìš° ëì— ê°ê° ì—´ê³¼ í–‰ì„ ì¶”ê°€í•œ ë’¤, 0ìœ¼ë¡œ ì±„ì›Œì„œ í¬ê¸°ë¥¼ ì¦ê°€ì‹œí‚¨ë‹¤.
-   ì¶œë ¥ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ì…ë ¥ ì´ë¯¸ì§€ì˜ í¬ê¸°ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ ì§ì ‘ ê³„ì‚°í•  í•„ìš” ì—†ì´ "same"ì´ë¼ëŠ” ê°’ì„ í†µí•´ ì…ë ¥ ì´ë¯¸ì§€ì˜ í¬ê¸°ì™€ ë™ì¼í•˜ê²Œ ë§ì¶œ ìˆ˜ ìˆë‹¤.

#### Pooling

-   Convolutoinì´ ì ìš©ëœ feature mapì˜ ì¼ì • ì—­ì˜ë³„ë¡œ í•˜ë‚˜ì˜ ê°’ì„ ì¶”ì¶œí•˜ì—¬ feature mapì˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì¸ë‹¤.
-   ë³´í†µì€ Convolution -> Relu activation -> Pooling ìˆœì„œë¡œ ì ìš©í•œë‹¤.
-   ë¹„ìŠ·í•œ featureë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ì—ì„œ ìœ„ì¹˜ê°€ ë‹¬ë¼ì§€ë©´ì„œ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” í˜„ìƒì„ ì¤‘í™”ì‹œí‚¬ ìˆ˜ ìˆê³ ,
    feature mapì˜ í¬ê¸°ê°€ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì—, ì—°ì‚° ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.
-   Max Poolingê³¼ Average Poolingì´ ìˆìœ¼ë©°, Max Poolingì€ ì¤‘ìš”ë„ê°€ ê°€ì¥ ë†’ì€ featureë¥¼ ì¶”ì¶œí•˜ê³ , Average Poolingì€ ì „ì²´ë¥¼ ë²„ë¬´ë ¤ì„œ ì¶”ì¶œí•œë‹¤.

#### ğŸš© ì •ë¦¬

-   Strideë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒê³¼ Poolingì„ ì ìš©í•˜ëŠ” ê²ƒì„ ì¶œë ¥ feature mapì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.
-   Convolution ì—°ì‚°ì„ ì§„í–‰í•˜ë©´ì„œ, feature mapì˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´, ìœ„ì¹˜ ë³€í™”ì— ë”°ë¥¸ featureì˜ ì˜í–¥ë„ë„ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì— ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆë‹¤.
-   Poolingì˜ ê²½ìš° íŠ¹ì • ìœ„ì¹˜ì˜ feature ê°’ì´ ì†ì‹¤ë˜ëŠ” ì´ìŠˆ ë“±ìœ¼ë¡œ ì¸í•˜ì—¬ ìµœê·¼ Advanced CNNì—ì„œëŠ” ë§ì´ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.
-   Classifierì—ì„œëŠ” Fully Connected Layerì˜ ì§€ë‚˜ì¹œ ì—°ê²°ë¡œ ì¸í•´ ë§ì€ íŒŒë¼ë¯¸í„°ê°€ ìƒì„±ë˜ë¯€ë¡œ ì˜¤íˆëŸ¬ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

-   ìœ„ì˜ ìƒí™©ì„ ëŒ€ë¹„í•˜ê¸° ìœ„í•´ Dropoutì„ ì‚¬ìš©í•´ì„œ Layerê°„ ì—°ê²°ì„ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë©° ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤. (ë‰´ëŸ°ì„ ë¹„í™œì„±í™” ì‹œí‚¤ëŠ” ì‘ì—….)

---

### CNN (RGB)

-   RGB ì˜ìƒì´ê¸° ë•Œë¬¸ì— í•„í„°ì˜ ê²½ìš° '3'ì´ ì ìš©ëœë‹¤.
-   input data ì™€ì˜ ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•´ Squeeze ë¥¼ ì‚¬ìš©í•œë‹¤ (<-> Unsqueeze)

---

### CNN Performance

-   CNN ëª¨ë¸ì„ ì œì‘í•  ë•Œ, ë‹¤ì–‘í•œ ê¸°ë²•ì„ í†µí•´ ì„±ëŠ¥ ê°œì„  ë° ê³¼ì í•© ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤.

#### Weight Initialization, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”

-   ì²˜ìŒ ê°€ì¤‘ì¹˜ë¥¼ ì–´ë–»ê²Œ ì¤„ ê²ƒì¸ì§€ë¥¼ ì •í•˜ëŠ” ë°©ë²•ì´ë©°, ì²˜ìŒ ê°€ì¤‘ì¹˜ë¥¼ ì–´ë–»ê²Œ ì„¤ì •í•˜ëŠëƒì— ë”°ë¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.

> 1. ì‚¬ë¹„ì—ë¥´ ê¸€ë¡œë¡œíŠ¸ ì´ˆê¸°í™”
>
> -   ê³ ì •ëœ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì´ì „ ì¸µì˜ ë…¸ë“œ ìˆ˜ì— ë§ê²Œ í˜„ì¬ ì¸µì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•œë‹¤.
> -   ì¸µë§ˆë‹¤ ë…¸ë“œ ê°œìˆ˜ë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ë”ë¼ë„ ì´ì— ë§ê²Œ ê°€ì¤‘ì¹˜ê°€ ì´ˆê¸°í™”ë˜ê¸° ë•Œë¬¸ì— ê³ ì •ëœ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ì´ìƒì¹˜ì— ë¯¼ê°í•˜ì§€ ì•Šë‹¤.
> -   í™œì„±í™” í•¨ìˆ˜ê°€ ReLUì¼ ë•Œ, ì¸µì´ ì§€ë‚  ìˆ˜ë¡ í™œì„±í™” ê°’ì´ ê³ ë¥´ì§€ ëª»í•˜ê²Œ ë˜ëŠ” ë¬¸ì œê°€ ìƒê²¨ì„œ, **ì¶œë ¥ì¸µì—ì„œë§Œ ì‚¬ìš©**í•œë‹¤.

> 2. ì¹´ì´ë° íˆ ì´ˆê¸°í™”
>
> -   ê³ ì •ëœ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì´ì „ ì¸µì˜ ë…¸ë“œ ìˆ˜ì— ë§ê²Œ í˜„ì¬ ì¸µì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•œë‹¤.
> -   ì¸µë§ˆë‹¤ ë…¸ë“œ ê°œìˆ˜ë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ë”ë¼ë„ ì´ì— ë§ê²Œ ê°€ì¤‘ì¹˜ê°€ ì´ˆê¸°í™”ë˜ê¸° ë•Œë¬¸ì— ê³ ì •ëœ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ì´ìƒì¹˜ì— ë¯¼ê°í•˜ì§€ ì•Šë‹¤.
> -   í™œì„±í™” í•¨ìˆ˜ê°€ ReLUì¼ ë•Œ, ì¶”ì²œí•˜ëŠ” ì´ˆê¸°í™” ë°©ë²•ìœ¼ë¡œì„œ, ì¸µì´ ê¹Šì–´ì§€ë”ë¼ë„ ëª¨ë“  í™œì„±ê°’ì´ ê³ ë¥´ê²Œ ë¶„í¬ëœë‹¤.

#### Batch Normalization, ë°°ì¹˜ ì •ê·œí™”

-   ì…ë ¥ ë°ì´í„° ê°„ì— ê°’ì˜ ì°¨ì´ê°€ ë°œìƒí•˜ë©´, ê°€ì¤‘ì¹˜ì˜ ë¹„ì¤‘ë„ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ì¸µì„ í†µê³¼í•  ìˆ˜ë¡ í¸ì°¨ê°€ ì‹¬í•´ì§„ë‹¤.  
    ì´ë¥¼ ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™(Internel Convariant Shift)ì´ë¼ê³  í•œë‹¤.
-   ê°€ì¤‘ì¹˜ì˜ ê°’ì˜ ë¯¸ì¤‘ì´ ë‹¬ë¼ì§€ë©´, íŠ¹ì • ê°€ì¤‘ì¹˜ì— ì¤‘ì ì„ ë‘ë©´ì„œ ê²½ì‚¬ í•˜ê°•ë²•ì´ ì§„í–‰ë˜ê¸° ë•Œë¬¸ì—,  
    ëª¨ë“  ì…ë ¥ê°’ì„ í‘œì¤€ ì •ê·œí™”í•˜ì—¬ ìµœì ì˜ parameterë¥¼ ë³´ë‹¤ ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•´ì•¼í•œë‹¤.
-   ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•  ë•Œ ë¯¼ê°ë„ë¥¼ ê°ì†Œì‹œí‚¤ê³ , í•™ìŠµ ì†ë„ë¥¼ ì¦ê°€ì‹œí‚¤ë©°, ëª¨ë¸ì„ ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤.

-   BNì€ activation function ì•ì— ì ìš©í•˜ë©´, weight ê°’ì€ í‰ê· ì´ 0, ë¶„ì‚°ì´ 1ì¸ ìƒíƒœë¡œ ì •ê·œë¶„í¬ê°€ ëœë‹¤.
-   ReLUê°€ activationìœ¼ë¡œ ì ìš©ë˜ë©´ ìŒìˆ˜ì— í•´ë‹¹í•˜ëŠ”(ì ˆë°˜ ì •ë„) ë¶€ë¶„ì´ 0ì´ ëœë‹¤.  
    ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ (ê°ë§ˆ)ì™€ (ë² íƒ€)ë¥¼ ì‚¬ìš©í•´ì„œ ìŒìˆ˜ë¶€ë¶„ì´ ëª¨ë‘ 0ì´ ë˜ëŠ” ê²ƒì„ ë§‰ì•„ì¤€ë‹¤.

#### Batch Size

-   batch sizeë¥¼ ì‘ê²Œ í•˜ë©´, ì ì ˆí•œ noiseê°€ ìƒê²¨ì„œ overfittingì„ ë°©ì§€í•˜ê²Œ ë˜ê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê³„ê¸°ê°€ ë  ìˆ˜ ìˆì§€ë§Œ, ë„ˆë¬´ ì‘ì•„ì„œëŠ” ì•ˆëœë‹¤.
-   batch sizeë¥¼ ë„ˆë¬´ ì‘ê²Œ í•˜ëŠ” ê²½ìš°ì—ëŠ” batchë‹¹ sample ìˆ˜ê°€ ì‘ì•„ì ¸ì„œ í›ˆë ¨ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë°ì— ë¶€ì¡±í•  ìˆ˜ ìˆë‹¤.
-   ë”°ë¼ì„œ êµ‰ì¥íˆ í¬ê²Œ ì£¼ëŠ” ê²ƒ ë³´ë‹¤ëŠ” ì‘ê²Œ ì£¼ëŠ” ê²ƒì´ ì¢‹ìœ¼ë©°, ì´ë¥¼ ë„ˆë¬´ ì‘ê²Œ ì£¼ì–´ì„œëŠ” ì•ˆëœë‹¤.  
    **ë…¼ë¬¸ì— ë”°ë¥´ë©´ 8ë³´ë‹¤ í¬ê³  32ë³´ë‹¤ ì‘ê²Œ ì£¼ëŠ” ê²ƒì´ íš¨ê³¼ì ì´ë¼ê³  í•œë‹¤.**

#### Weight Regularization (ê°€ì¤‘ì¹˜ ê·œì œ), Weight Decay (ê°€ì¤‘ì¹˜ ê°ì†Œ)

-   loss functionì€ loss ê°’ì´ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ updateí•œë‹¤.
-   í•˜ì§€ë§Œ, lossë¥¼ ì¤„ì´ëŠ” ë°ì—ë§Œ ì‹ ê²½ì“°ê²Œ ë˜ë©´, íŠ¹ì • ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì»¤ì§€ë©´ì„œ ì˜¤íˆë ¤ ë‚˜ìœ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.
-   ê¸°ì¡´ ê°€ì¤‘ì¹˜ì— íŠ¹ì • ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì—¬ loss functionì˜ ì¶œë ¥ ê°’ê³¼ ë”í•´ì£¼ë©´ loss functionì˜ ê²°ê³¼ë¥¼ ì–´ëŠì •ë„ ì œì–´í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
-   ë³´í†µ íŒŒë¼ë¯¸í„°ê°€ ë§ì€ Dense Layerì—ì„œ ë§ì´ ì‚¬ìš©ë˜ê³  ê°€ì¤‘ì¹˜ ê·œì œë³´ë‹¤ëŠ” loss functionì— ê·œì œë¥¼ ê±¸ì–´ ê°€ì¤‘ì¹˜ë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ì›ë¦¬ì´ë‹¤.
-   kerenlregularizer íŒŒë¼ë¯¸í„°ì—ì„œ l1, l2ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤.

---

### ì‹¤ì œ ì˜ìƒ ë°ì´í„°ë¥¼ train, validation, test ë°ì´í„° ë¶„ë¦¬

-   ì•„ë˜ code 6~8 ì°¸ì¡°

#### <div id="cnn-code">CNN Code</div>

<details>
    <summary>1. Funtional API ë¥¼ ì´ìš©í•œ CNN model êµ¬ì„±.</summary>

        from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten
        from tensorflow.keras.models import Model

        INPUT_SIZE = 28

        # ì…ë ¥ í…ì„œ ì •ì˜: 28x28 í¬ê¸°ì˜ gray ì´ë¯¸ì§€
        # ë”°ë¼ì„œ Input í•­ëª©ì— 3ì°¨ì›ìœ¼ë¡œ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜ë¥¼ ì…ë ¥í•œë‹¤
        (ë‹¨, 3ì°¨ì›ìœ¼ë¡œ ë‚˜ì—´ ë˜ì–´ìˆì„ ê²½ìš° ì±„ë„ ìˆ˜ ì´ì ê°œìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.)
        input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE, 1))

        ## parms ì´ ê°œìˆ˜
        ## input = 1
        ## kernel = 3 * 3 = 9
        ## filter = 16
        ## 9 * 16 + 16 = 160

        # Conv2DëŠ” 2ì°¨ì› í•©ì„±ê³±(Convolution) ë ˆì´ì–´ë¥¼ ì˜ë¯¸í•˜ë©° feature mapì„ ìƒì„±í•˜ê¸° ìœ„í•œ ë ˆì´ì–´

        x = Conv2D(filters = 16, kernel_size= 3, strides=1, padding='same',activation='relu')(input_tensor)

        ## input = 16
        ## kernel = 4 * 4 = 16
        ## filter = 32
        ## 16 * 16 * 32 + 32 = 8224

        x = Conv2D(filters = 32, kernel_size= 4, strides=1, padding='same',activation='relu')(x)

        # input = 32
        # kernel = 4 * 4 = 16
        # filter = 64
        # 32 * 16 * 64 + 64 = 32832

        x = Conv2D(filters = 64, kernel_size= 4, strides=1,activation='relu')(x)

        x = MaxPool2D(2)(x)

        # ì…ë ¥ì¸µ
        x = Flatten()(x)
        # íˆë“ ì¸µ
        x = Dense(50, activation='relu')(x)
        # íˆë“ ì¸µ
        x = Dense(20, activation='relu')(x)
        # ì¶œë ¥ì¸µ
        output = Dense(10, activation='softmax')(x)

        model = Model(inputs= input_tensor, outputs = output)
        model.summary()

</details>

<details>
    <summary>2. Dropout (ë‰´ëŸ° ë¹„í™œì„±í™” ë¹„ìœ¨).</summary>

        from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, Dropout
        from tensorflow.keras.models import Model

        INPUT_SIZE = 28

        input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE, 1))


        x = Conv2D(filters = 16, kernel_size= 3, strides=1, padding='same',activation='relu')(input_tensor)
        x = Conv2D(filters = 32, kernel_size= 4, strides=1, padding='same',activation='relu')(x)
        x = Conv2D(filters = 64, kernel_size= 4, strides=1,activation='relu')(x)

        x = MaxPool2D(2)(x)

        x = Flatten()(x)

        # Dropout(rate=ë¹„í™œì„±í™” í•  ë¹„ìœ¨ ì„ íƒ)

        x = Dropout(rate=0.5)(x)
        x = Dense(50, activation='relu')(x)
        x = Dense(20, activation='relu')(x)
        output = Dense(10, activation='softmax')(x)

        model = Model(inputs= input_tensor, outputs = output)
        model.summary()

</details>

<details>
    <summary>Tip.validation_split</summary>

    # compile ì§„í–‰ ì‹œ ë³„ë„ë¡œ validation ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê³  validation_splitì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

    model.fit(x=train_iamges, y=train_target, batch_size=8, epochs=10,
    validation_split=0.2)

</details>

<details>
    <summary>3. RGB ì˜ìƒ CNN ëª¨ë¸ ìƒì„±</summary>

    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, Input, Activation
    from tensorflow.keras.callbacks import Callback

    INPUT_SIZE = 32

    # RGB ì˜ìƒì´ê¸° ë•Œë¬¸ì— ìµœì´ˆ 3ê°œì˜ í•„í„°ë¥¼ ë„£ëŠ”ë‹¤.
    input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE, 3))

    # padding default == valid
    x = Conv2D(filters = 32, kernel_size=5, padding='valid', activation='relu')(input_tensor)
    x = Conv2D(filters = 32, kernel_size=3, padding='same', activation='relu')(x)
    x = MaxPooling2D(2)(x)

    x = Conv2D(filters = 64, kernel_size=3, padding='same', activation='relu')(x)
    x = Conv2D(filters = 64, kernel_size=3, padding='same')(x)
    # CNN performance ì˜ ë°°ì¹˜ ì •ê·œí™” ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í™œì„±í•¨ìˆ˜ë¥¼ ë”°ë¡œ ì ìš©í•œë‹¤.
    x = Activation('relu')(x)
    x = MaxPooling2D(2)(x)

    x = Conv2D(filters = 128, kernel_size=3, padding='same', activation='relu')(x)
    x = Conv2D(filters = 128, kernel_size=3, padding='same')(x)
    x = Activation('relu')(x)
    x = MaxPooling2D(2)(x)

    x = Flatten(name='classifier_A00_Flatten')(x)
    x = Dropout(name='classifierA_DropOut01', rate=0.5)(x)
    x = Dense(300, activation='relu', name='classifierAD01')(x)
    x = Dropout(name='classifierA_DropOut02', rate=0.5)(x)
    output = Dense(10, activation='softmax', name='output')(x)


    model = Model(inputs = input_tensor, outputs = output)
    model.summary()

</details>

<details>
    <summary>4. keras.losses SparseCategoricalCrossentropy(ì›-í•« ì¸ì½”ë”© í›„ ì†ì‹¤í•¨ìˆ˜ í™•ì¸)</summary>

    from tensorflow.keras.optimizers import Adam
    # from tensorflow.keras.losses import CategoricalCrossentropy
    # ë‚´ê°€ ì›-í•« ì¸ì½”ë”©ì„ í•˜ì§€ì•Šê³  í•¨ìˆ˜ ë‚´ë¶€ì ìœ¼ë¡œ ì›-í•« ì¸ì½”ë”©ì„ ì‹œì¼œì¤€ë‹¤.
    from tensorflow.keras.losses import SparseCategoricalCrossentropy

    model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(), metrics = ['acc'])

</details>

<details>
    <summary>5. CNN Performance ì ìš© ëª¨ë¸(kernel_initializer(ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”), BatchNormalization(ë°°ì¹˜ ì •ê·œí™”), GlobalAveragePooling2D), kernel_regularizer(ê°€ì¤‘ì¹˜ ê·œì œ)</summary>

    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Activation, Dropout, GlobalAveragePooling2D, BatchNormalization
    from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
    from tensorflow.keras.regularizers import l1, l2

    INPUT_SIZE = 32

    input_tensor = Input(shape=(INPUT_SIZE,INPUT_SIZE,3))

    # alphaë¥¼ í¬ê²Œ í•  ìˆ˜ë¡ Weightê°’ì„ ì‘ê²Œ ë§Œë“¤ì–´ì„œ ê³¼ì í•©ì„ ê°œì„ í•  ìˆ˜ ìˆê³ ,
    # alphaë¥¼ ì‘ê²Œ í•  ìˆ˜ë¡ Weightì˜ ê°’ì´ ì»¤ì§€ì§€ë§Œ, ì–´ëŠ ì •ë„ ìƒì‡„í•˜ë¯€ë¡œ ê³¼ì†Œì í•©ì„ ê°œì„ í•  ìˆ˜ ìˆë‹¤.
    # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì¹´ì´ë° íˆ ì´ˆê¸°í™”(he_normal))
    x = Conv2D(filters=64, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(input_tensor)
    # ë°°ì¹˜ ì •ê·œí™”
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(filters=64, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling2D(2)(x)

    x = Conv2D(filters=128, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(filters=128, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling2D(2)(x)

    x = Conv2D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling2D(2)(x)

    x = GlobalAveragePooling2D()(x)
    x = Dropout(rate=0.5)(x)
    x = Dense(300, activation='relu', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
    # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì‚¬ë¹„ì—ë¥´ ê¸€ë¡œë¡œíŠ¸ ì´ˆê¸°í™” (glorot_normal))
    x = Dropout(rate=0.5)(x)
    output = Dense(10, activation='softmax', kernel_initializer='glorot_normal')(x)

    model = Model(inputs= input_tensor, outputs = output)
    model.summary()

</details>

<details>
    <summary>6. â­ï¸ (ì‹¤ì œ ì´ë¯¸ì§€)ë™ë¬¼ ì´ë¯¸ì§€ ì˜ìƒ train, val, test êµ¬ë¶„(MAC, Window)</summary>

    # ì‚¬ì „ì— ì •ì˜ëœ ëª…ì¹­ì´ ìˆê¸° ë•Œë¬¸ì— í•´ë‹¹ íŒŒì¼ì„ ë¶ˆë¡œì˜¤ëŠ” ë©”ì†Œë“œ
    with open('../d_cnn/datasets/animals/translate.py') as f:
        content = f.readline().strip()
        # print(content)
        # ë¬¸ìì—´ ì•ˆì— ìˆëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ì •ìƒì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ë©”ì†Œë“œ eval
        contents1 = eval(content[content.index("{"):content.index("}") + 1])

        # key, valueê°€ ë’¤ì§‘ì–´ì ¸ ìˆëŠ” ìƒíƒœì´ê¸° ë•Œë¬¸ì— ë”•ì…”ë„ˆë¦¬ì˜ itemsë¥¼ ê°€ì ¸ì™€ì„œ key:value ë°˜ì „
        contents2 = {v: k for k, v in contents1.items()}

    print(contents1, contents2, sep='\n\n')

---

    from glob import glob
    import os

    root = '../d_cnn/datasets/animals/original/'

    # glob í•¨ìˆ˜ì˜ ê²½ìš° íŒŒì¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    # osì˜ root ê²½ë¡œì— ìˆëŠ” ëª¨ë“  íŒŒì¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ì—°ê²°.
    directories = glob(os.path.join(root, '*'))
    print(directories)

    for directory in directories:
        # í”Œë«í¼ ë…ë¦½ì ìœ¼ë¡œ ë””ë ‰í† ë¦¬ ì´ë¦„ ì¶”ì¶œ
        # basname (ë¦¬ëˆ…ìŠ¤ì—ì„œ íŒŒì¼ëª…ì´ë‚˜ í™•ì¥ìë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ëª…ë ¹ì–´)
        old_name = os.path.basename(directory)

        # í•´ë‹¹ ì˜ˆì™¸ì²˜ë¦¬ëŠ” translate.py í•­ëª©ì— key, valueê°€ ì¤‘ë³µìœ¼ë¡œ ë˜ì–´ ìˆì–´ ì‘ì„±
        try:
            new_name = contents1[old_name]
        except KeyError:
            new_name = contents2.get(old_name, old_name)  # old_nameì´ contents2ì— ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
        new_directory = os.path.join(root, new_name)
        os.rename(directory, new_directory)

    +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    <!-- window ì…ë‹ˆë‹¤ -->
    from glob import glob
    import os

    root = './datasets/animals/original/'

    directories = glob(os.path.join(root, '*'))

    for directory in directories:
    # ìœˆë„ìš°ì˜ ê²½ìš° íŒŒì¼ëª… ì•ì´ â‚© ë¡œ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— \\ ë¥¼ í†µí•˜ì—¬ í•´ë‹¹ íŒŒì¼ëª…ì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
        try:
            os.rename(directory, os.path.join(root, contents1[directory[directory.rindex('\\') + 1:]]))
        except KeyError as e:
            os.rename(directory, os.path.join(root, contents2[directory[directory.rindex('\\') + 1:]]))

---

    root = '../d_cnn/datasets/animals/original/'

    # í•´ë‹¹ ê²½ë¡œì— ìˆëŠ” ì „ì²´ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥
    directories = glob(os.path.join(root, '*'))
    directory_names = []

    # ë°˜ë³µì„ ì´ìš©í•œ í•´ë‹¹ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ëª…ì„ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    for directory in directories:
        directory_names.append(os.path.basename(directory))

    print(directory_names)

---

    root = '../d_cnn/datasets/animals/original/'

    for name in directory_names:
        for i, file_name in enumerate(os.listdir(os.path.join(root, name))):
            old_file = os.path.join(root + name + '/', file_name)
            new_file = os.path.join(root + name + '/', name + str(i + 1) + '.png')

            # ê¸°ì¡´ì— ìˆë˜ íŒŒì¼ëª…ì„ í•´ë‹¹ root ë””ë ‰í† ë¦¬ì˜ ì´ë¦„ìœ¼ë¡œ ë³€ê²½ í›„ ë’¤ì— ë°˜ë³µ ì‹œ ì¦ê°€ë˜ëŠ” ìˆ«ì ì…ë ¥
            os.rename(old_file, new_file)

---

    from tensorflow.keras.preprocessing.image import ImageDataGenerator

    # ImageDataGenerator ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
    # ëª¨ë“  ì´ë¯¸ì§€ì˜ í”½ì…€ê°’ì„ 1/255 ë¡œ ë‚˜ëˆ„ì–´ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜ í›„ image_data_generator ìƒì„±
    image_data_generator = ImageDataGenerator(rescale=1./255)

    # flow_from_directory ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    # flow_from_directoryëŠ” ImageDataGeneratorì˜ ë©”ì†Œë“œë¡œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ 
    # ì‹¤ì‹œê°„ìœ¼ë¡œ ì¦ê°• ë° ì „ì²˜ë¦¬ í•˜ëŠ”ë° ì‚¬ìš©
    generator = image_data_generator.flow_from_directory(
        root,                 # ì´ë¯¸ì§€ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ì˜ ê²½ë¡œ
        target_size=(150, 150),  # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ (150, 150) í¬ê¸°ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.
        batch_size=32,           # ë°°ì¹˜ í¬ê¸°ë¥¼ 32ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        class_mode='categorical' # í´ë˜ìŠ¤ ëª¨ë“œë¥¼ 'categorical'ë¡œ ì„¤ì •í•˜ì—¬ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    )

    # ìƒì„±ëœ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    # ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ë°œê²¬ëœ í´ë˜ìŠ¤ì˜ ì´ë¦„ê³¼ ì¸ë±ìŠ¤ë¥¼ ë§¤í•‘í•œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    print(generator.class_indices)

---

    import pandas as pd
    from sklearn.model_selection import train_test_split

    # flow_from_directoryë¥¼ ì´ìš©í•œ ë©”ì†Œë“œë¡œ í•´ë‹¹ íŒŒì¼ì„ ì§ì ‘ ë¡œë“œí•˜ì—¬ ê²½ë¡œì™€ categoryë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    a_df = pd.DataFrame({'file_paths': generator.filepaths, 'targets': generator.classes})
    a_df

---

    # train, validation, test ë°ì´í„° ë¶„ë¦¬
    X_train, X_test, y_train, y_test =\
    train_test_split(a_df.file_paths, a_df.targets, stratify=a_df.targets, test_size=0.2, random_state=124)

    print(y_train.value_counts())
    print(y_test.value_counts())

    X_train, X_val, y_train, y_val = \
    train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=124)

    print(y_train.value_counts())
    print(y_val.value_counts())

---

    # ê¸°ì¡´ 1ê°œì˜ í´ë”ì— ìˆëŠ” ì´ë¯¸ì§€ë“¤ì„ train, validation, test ì˜ìƒìœ¼ë¡œ ë””ë ‰í† ë¦¬ ë‚˜ëˆ ì„œ copy
    import shutil

    root = '../d_cnn/datasets/animals/'

    for file_path in X_train:
        # animal_dirì„ ê²½ë¡œ êµ¬ë¶„ìë¡œ ë¶„í• í•˜ì—¬ ì¶”ì¶œ
        # íŒŒì¼ ê²½ë¡œì—ì„œ directory ë¥¼ ì¶”ì¶œí•˜ë ¤ë©´ dirname ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©
        animal_dir = file_path[len(os.path.join(root, 'original')) + 1:file_path.rindex('/')]
        destination = os.path.join(root, 'train', animal_dir)

        # destination ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        if not os.path.exists(destination):
            os.makedirs(destination)

        # íŒŒì¼ì„ destination ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
        shutil.copy2(file_path, destination)

---

    import shutil

    root = '../d_cnn/datasets/animals/'

    for file_path in X_val:
        # animal_dirì„ ê²½ë¡œ êµ¬ë¶„ìë¡œ ë¶„í• í•˜ì—¬ ì¶”ì¶œ
        animal_dir = file_path[len(os.path.join(root, 'original')) + 1:file_path.rindex('/')]
        destination = os.path.join(root, 'validation', animal_dir)

        # destination ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        if not os.path.exists(destination):
            os.makedirs(destination)

        # íŒŒì¼ì„ destination ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
        shutil.copy2(file_path, destination)

---

    root = '../d_cnn/datasets/animals/'

    for file_path in X_test:
        # animal_dirì„ ê²½ë¡œ êµ¬ë¶„ìë¡œ ë¶„í• í•˜ì—¬ ì¶”ì¶œ
        animal_dir = file_path[len(os.path.join(root, 'original')) + 1:file_path.rindex('/')]
        destination = os.path.join(root, 'test', animal_dir)

        # destination ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        if not os.path.exists(destination):
            os.makedirs(destination)

        # íŒŒì¼ì„ destination ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
        shutil.copy2(file_path, destination)

</details>
</div>
